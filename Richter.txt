
1. CLR Hosting


When developing the CLR, Microsoft implemented it as a COM server contained inside a DLL;
that is, Microsoft defined a standard COM interface for the CLR and assigned GUIDs to this interface
and the COM server. When you install the .NET Framework, the COM server representing the CLR is
registered in the Windows registry just as any other COM server would. If you want more information
about this topic, refer to the MetaHost.h C++ header file that ships with the .NET Framework SDK.
This header file defines the GUIDs and the unmanaged ICLRMetaHost interface definition.

2. What happens when a managed executable starts.

By default, when a managed executable starts, the shim examines the executable file and extracts
the information indicating the version of the CLR that the application was built and tested
with.
Initialize and start the CLR.
Load an assembly and execute code in it.

3. How does Windows know that a binary is a .NET application?

Normally .exe files are executed by Windows by looking at the PE-Header. 
This PE-Header says how it should be loaded into memory, 
what dependencies it has, and where the entry point is.

4. Where is the entry-point of a .NET application?

Well, your application is in some IL-code. 
Executing that directly will clearly lead to a crash. 
It is not the IL-code that should start executing, 
but the .NET runtime, which eventually should load the IL-code and execute it.

In newer versions of Windows, .NET comes preinstalled, 
and Windows has built-in support for recognizing a .NET application. 
This can be done by simply looking in the PE-Header present in all executables and DLLs. 
In older versions of Windows, execution is passed to an entry point 
where boot-strapper code is located. 

The boot-strapper, which is native code, uses an unmanaged CLR Hosting API, 
to start the .NET runtime inside the current process 
and launch the real program which is the IL-code.

5. Hosting the CLR in an unmanaged app

When you start the .NET runtime inside a native process, 
that native application becomes a host for the runtime. 
This lets you add .NET capabilities to your native applications.

#include <metahost.h>
#include <mscoree.h>
#pragma comment(lib, "mscoree.lib")

ICLRMetaHost    *pMetaHost     = nullptr;
ICLRRuntimeHost *pRuntimeHost  = nullptr;
ICLRRuntimeInfo *pRuntimeInfo  = nullptr;
HRESULT hr;

hr = CLRCreateInstance(CLSID_CLRMetaHost, IID_ICLRMetaHost, (LPVOID*)&pMetaHost);
hr = pMetaHost->GetRuntime(runtimeVersion, IID_PPV_ARGS(&pRuntimeInfo));
hr = pRuntimeInfo->GetInterface(CLSID_CLRRuntimeHost,IID_PPV_ARGS(&pRuntimeHost));
hr = pRuntimeHost->Start();

_AppDomain* pCurrentDomain = nullptr;
hr = pRuntimeHost->GetDefaultDomain(&pCurrentDomain);
pCurrentDomain.ExecuteAssembly(assemblyFilename);


Now the runtime is running, but it hasn't got any loaded user code yet. 
Some internal thread scheduler and garbage collector are surely running, 
because they are part of the CLR runtime.


To be able to register a new AppDomainManager, 
we will need an interface called ICLRControl. 
This interface contains a method SetAppDomainManagerType, 
which loads your managed implementation of the AppDomainManager.


ICLRControl* pCLRControl = nullptr;
hr = pRuntimeHost->GetCLRControl(&pCLRControl);
LPCWSTR assemblyName = L"SampleAppDomainManager";
LPCWSTR appDomainManagerTypename = L"SampleAppDomainManager.CustomAppDomainManager";
hr = pCLRControl->SetAppDomainManagerType(assemblyName, appDomainManagerTypename);


[GuidAttribute("0C19678A-CE6C-487B-AD36-0A8B7D7CC035"), ComVisible(true)]
public sealed class CustomAppDomainManager : AppDomainManager, ICustomAppDomainManager
{
  public CustomAppDomainManager()
  {
     System.Console.WriteLine("*** Instantiated CustomAppDomainManager");
  }

  public override void InitializeNewDomain(AppDomainSetup appDomainInfo)
  {
     System.Console.WriteLine("*** InitializeNewDomain");
     this.InitializationFlags = AppDomainManagerInitializationOptions.RegisterWithHost;
  }

  public override AppDomain CreateDomain(string friendlyName,
          Evidence securityInfo, AppDomainSetup appDomainInfo)
  {
     var appDomain = base.CreateDomain(friendlyName, securityInfo, appDomainInfo);
     System.Console.WriteLine("*** Created AppDomain {0}", friendlyName);
     return appDomain;
  }
}

Before any managed code can be executed, 
the host must load and initialize the common language runtime.
All hosts start with an unmanaged stub because the runtime is not yet running in the process.

After loading and initializing the common language runtime, 
the host must make the transition from unmanaged to managed code 
in order to execute managed hosting code and user code.

After a host has determined where domain boundaries lie, 
based on the criteria described in the previous section, 
the host uses the CreateDomain method of the System.AppDomain type 
to create domains in which to run user code. 
Each application domain contains a collection of name/value pairs 
in which a host can store information about a domain. 
The name/value pairs are passed as a parameter to CreateDomain.


6. AppDomains.


When the CLR COM server initializes, it creates an AppDomain. An AppDomain is a logical container
for a set of assemblies. The first AppDomain created when the CLR is initialized is called the default
AppDomain; this AppDomain is destroyed only when the Windows process terminates.

In addition to the default AppDomain, a host using either unmanaged COM interface methods or
managed type methods can instruct the CLR to create additional AppDomains. The whole purpose of
an AppDomain is to provide isolation. Here are the specific features offered by an AppDomain:

- Objects created by code in one AppDomain cannot be accessed directly by code in another

AppDomain When code in an AppDomain creates an object, that object is “owned”
by that AppDomain. In other words, the object is not allowed to live beyond the lifetime of
the AppDomain whose code constructed it. Code in other AppDomains can access another
AppDomain’s object only by using marshal-by-reference or marshal-by-value semantics. This
enforces a clean separation and boundary because code in one AppDomain can’t have a
direct reference to an object created by code in a different AppDomain.

-AppDomains can be unloaded The CLR doesn’t support the ability to unload a single assembly

from an AppDomain. However, you can tell the CLR to unload an AppDomain, which
will cause all of the assemblies currently contained in it to be unloaded as well.

-AppDomains can be individually secured 
When created, an AppDomain can have a permission
set applied to it that determines the maximum rights granted to assemblies running
in the AppDomain. This allows a host to load some code and be ensured that the code cannot
corrupt or read important data structures used by the host itself.

Important A great feature of Windows is that it runs each application in its own process
address space. This ensures that code in one application cannot access code or data in use
by another application. Process isolation prevents security holes, data corruption, and other
unpredictable behaviors from occurring, making Windows and the applications running
on it robust. Unfortunately, creating processes in Windows is very expensive. The Win32
CreateProcess function is very slow, and Windows requires a lot of memory to virtualize
a process’s address space.

However, if an application consists entirely of managed code that is verifiably safe and
doesn’t call out into unmanaged code, there are no problems related to running multiple
managed applications in a single Windows process. And AppDomains provide the isolation
required to secure, configure, and terminate each of these applications.

each type object
in the loader heap has a method table, and each entry in the method table points to JIT-compiled native
code if the method has been executed at least once.

Furthermore, as code in an AppDomain calls methods defined by a type, the method’s
Intermediate Language (IL) code is JIT-compiled, and the resulting native code is associated with each
AppDomain;

Some assemblies are expected to be used by several AppDomains. MSCorLib.dll is the best example.
This assembly contains System.Object, System.Int32, and all of the other types that are so
integral to the .NET Framework. This assembly is automatically loaded when the CLR initializes, and all
AppDomains share the types in this assembly. To reduce resource usage, MSCorLib.dll is loaded in an
AppDomain-neutral fashion;



7. Accessing Objects Across AppDomain Boundaries.

Marshal by reference communication between two different domains.

{
   AppDomain adCallingThreadDomain = Thread.GetDomain();
   String exeAssembly = Assembly.GetEntryAssembly().FullName;
   
   AppDomain ad2 = AppDomain.CreateDomain("AD #2", null, null);
   mbrt = (MarshalByRefType)ad2.CreateInstanceAndUnwrap(exeAssembly, "MarshalByRefType");
   
   // Prove that we got a reference to a proxy object
   Console.WriteLine("Is proxy={0}", RemotingServices.IsTransparentProxy(mbrt));

   // This looks like we're calling a method on MarshalByRefType but we're not.
   // We're calling a method on the proxy type. The proxy transitions the thread
   // to the AppDomain owning the object and calls this method on the real object.
   mbrt.SomeMethod();
}


However, a one-to-one
correspondence doesn’t exist between threads and AppDomains. AppDomains are a CLR feature;
Windows knows nothing about AppDomains. Because multiple AppDomains can be in a single
Windows process, a thread can execute code in one AppDomain and then execute code in another
AppDomain. From the CLR’s perspective, a thread is executing code in one AppDomain at a time.


The new AppDomain
will have its very own loader heap, which will be empty because there are currently no assemblies
loading into the new AppDomain. When you create an AppDomain, the CLR does not create any
threads in this AppDomain; no code runs in the AppDomain unless you explicitly have a thread call
code in the AppDomain.

When a source AppDomain wants to send or return the reference of an object to a destination
AppDomain, the CLR defines a proxy type in the destination AppDomain’s loader heap. This proxy
type is defined using the original type’s metadata, and therefore, it looks exactly like the original type;
it has all of the same instance members (properties, events, and methods).

8. Domain unloading.

One of the great features of AppDomains is that you can unload them. Unloading an AppDomain
causes the CLR to unload all of the assemblies in the AppDomain, and the CLR frees the AppDomain’s
loader heap as well.

- The CLR suspends all threads in the process that have ever executed managed code.
- The CLR forces any threads that have the unloading
  AppDomain on their stack to throw a ThreadAbortException 
  (resuming the thread’s execution).

1. 

This causes the threads to unwind, 
executing any finally blocks on their way out so
that cleanup code executes.

This causes the threads to unwind, executing any finally blocks on their way out so
that cleanup code executes.

If no code catches the ThreadAbortException, it will eventually
become an unhandled exception that the CLR swallows;

the thread dies, but the process is allowed to continue running. 
This is unusual, because for all other unhandled exceptions, the
CLR kills the process.

An aborting thread is
allowed to finish executing these code blocks and then, at the end of the code
block, the CLR forces the thread to throw a ThreadAbortException.


2. 
After all threads discovered in step 2 have left the AppDomain, the CLR then walks the heap
and sets a flag in each proxy object that referred to an object created by the unloaded AppDomain.
These proxy objects now know that the real object they referred to is gone. If any
code now calls a method on an invalid proxy object, the method will throw an AppDomainUnloadedException.

3. 
The CLR forces a garbage collection to occur, reclaiming the memory used by any objects that
were created by the now unloaded AppDomain. The Finalize methods for these objects are
called, giving the objects a chance to clean themselves up properly.

4.
The CLR resumes all of the remaining threads. The thread that called AppDomain.Unload will
now continue running; calls to AppDomain.Unload occur synchronously.

5. 
The CLR forces a garbage collection to occur, reclaiming the memory used by any objects that
were created by the now unloaded AppDomain. The Finalize methods for these objects are
called, giving the objects a chance to clean themselves up properly.

6.
The CLR resumes all of the remaining threads. The thread that called AppDomain.Unload will
now continue running; calls to AppDomain.Unload occur synchronously.


When Windows initializes a process by using a managed EXE file, Windows loads the shim,
and the shim examines the CLR header information contained in the application’s assembly 
(the EXE file).


9. Executable application.

When Windows initializes a process by using a managed EXE file, Windows loads the shim,
and the shim examines the CLR header information contained in the application’s assembly 
(the EXE file). 

The header information indicates the version of the CLR 
that was used to build and test the application. 

The shim uses this information to determine which version of the CLR to load into the
process. After the CLR loads and initializes, it again examines the assembly’s CLR header to determine
which method is the application’s entry point (Main). The CLR invokes this method, and the application
is now up and running.

releases all of the unmanaged COM objects held by the CLR.


10. How does Silverlight launch CLR?

1. Loads the Silverlight CLR (CoreClr.dll) in your browser.
2. Each Silverlight control on the page runs in its own AppDomain.
3. When the user closes a tab or navigates to another website, 
   any Silverlight controls no longer in use have their AppDomains unloaded.

11. How does ASP.NET and XML Web Services Applications?

ASP.NET is implemented as an ISAPI DLL (implemented in ASPNet_ISAPI.dll).
The first time a client requests a URL handled 
by the ASP.NET ISAPI DLL, ASP.NET loads the CLR.

If it is, ASP.NET tells the CLR to create a new AppDomain for this web application;

ASP.NET then tells the CLR to load the assembly that contains
the type exposed by the web application into this new AppDomain, creates an instance of this type,
and starts calling methods in it to satisfy the client’s web request.

The methods will already be JIT-compiled into native code,
so the performance of processing all subsequent client requests is excellent.

If a client makes a request of a different web application, 
ASP.NET tells the CLR to create a new AppDomain.

This new AppDomain is typically created inside the same worker process as the other
AppDomains.


12. How does Microsoft SQL Server launch CLR?

13. What does the CLR host do when the thread is running for a long period 
of time and we need to stop it?

When the host originally received the client’s request, it recorded the time. If the untrusted
code doesn’t respond to the client in some administrator-set amount of time, the host calls
Thread’s Abort method, asking the CLR to stop the thread pool thread, forcing it to throw a
ThreadAbortException.

14. What is the process of C# code compilation?
 - Then you use the corresponding compiler to check the syntax and analyze the source code.
 - Builds the C# code into managed module.

A managed module is a standard 32-bit Windows portable
executable (PE32) file or a standard 64-bit Windows portable executable (PE32+) 
file that requires the CLR to execute.

Extentions that have been made for .NET metadata.
CLR sections:

1. The standard Windows PE file header, which is similar to the Common Object File Format
   (COFF) header. This header also indicates the type of file: GUI, CUI, or DLL, and
   contains a time stamp indicating when the file was built. For modules that contain
   native CPU code, this header contains information about the native CPU code.
   
2. CLR Header.
   Contains the information 
   (interpreted by the CLR and utilities) that makes this a managed
   module. The header includes the version of the CLR required.
   
3. Metadata section. Exported types. 
   Every managed module contains metadata tables. There are two main types of tables:
   tables that describe the types and members defined in your source code and tables that
   describe the types and members referenced by your source code.   
   In addition, metadata also has tables indicating what the
   managed module references, such as imported types and their members.
   Metadata is a superset of older technologies such 
   as COM’s Type Libraries and Interface Definition Language (IDL) files. 
   The important thing to note is that CLR metadata is far more complete.
   And, unlike Type Libraries and IDL, metadata is always associated with the file 
   that contains the IL code. 
   In fact, the metadata is always embedded in the same EXE/DLL as the code, 
   making it impossible to separate the two
   

4. Intermediate Language (IL).  Common intermediate Language .NET CLR. 
   Code the compiler produced as it compiled the source code. 
   At run time, the CLR compiles the IL into native CPU instructions.
   

15. What are benefits from Metadata?

- Compilers can read metadata directly from managed modules.
- Microsoft Visual Studio uses metadata to help you write code. 
  Its IntelliSense feature parses metadata to tell you 
  what methods, properties, events, and fields a type offers.
- The CLR’s code verification process uses metadata to ensure 
  that your code performs only “type-safe” operations.
- Metadata allows an object’s fields to be serialized into a memory block.
- Metadata allows the garbage collector to track the lifetime of objects. 
  For any object, the garbage collector can determine the type of the object 
  and, from the metadata, know which fields within that object refer to other objects.

  ###############################################################
  
16. How is IL code converted to CPU native code?

To execute a method, its IL must first be converted to native CPU instructions. 
This is the job of the CLR’s JIT (just-in-time) compiler.

When Main makes its first call to WriteLine, the JITCompiler function is called. The JITCompiler
function is responsible for compiling a method’s IL code into native CPU instructions.

JITCompiler

1. In the assembly that implements the type(Console), 
   look up the method (WriteLine) being called in the metadata.
2. From the metadata, get the IL for this method.
3. Allocate a block of memory.
4. Compile the IL into native CPU instructions;
   the native code is saved in the memory allocated in step 3.
5. Modify the method’s entry in the Type’s table so that 
   it now points to the memory block allocated in step 3.
6. Jump to the native code contained inside the memory block.

Main now calls WriteLine a second time. This time, the code for WriteLine has already been
verified and compiled. So the call goes directly to the block of memory, skipping the JITCompiler
function entirely.

A performance hit is incurred only the first time a method is called. All subsequent calls to the
method execute at the full speed of the native code because verification and compilation to native
code don’t need to be performed again.

The JIT compiler stores the native CPU instructions in dynamic memory. This means that the compiled
code is discarded when the application terminates. So if you run the application again in the
future or if you run two instances of the application simultaneously (in two different operating system
processes), the JIT compiler will have to compile the IL to native instructions again.

###############################################################

17. When managed code compilation can overperform unmanaged code?

- A JIT compiler can determine if the application is running on an Intel Pentium 4 CPU and produce
  native code that takes advantage of any special instructions offered by the Pentium 4.
  Usually, unmanaged applications are compiled for the lowest-common-denominator CPU and
  avoid using special instructions that would give the application a performance boost.
  
- A JIT compiler can determine when a certain test is always false 
  on the machine that it is running on.

- The CLR could profile the code’s execution and recompile the IL into native code 
  while the application runs.

###############################################################  
 
18. What does NGen.exe do?

This tool compiles all of an assembly’s IL code into native code and saves the
resulting native code to a file on disk. At run time, when an assembly is loaded, the CLR automatically
checks to see whether a precompiled version of the assembly also exists, and if it does, the CLR loads
the precompiled code so that no compilation is required at run time.

###############################################################

19. What does CLR verification do?

- Verification examines the high-level IL code and
  ensures that everything the code does is safe.
  
- Verification checks that every method is
  called with the correct number of parameters,
  
- that each parameter passed to every method is of the correct type, 

- that every method’s return value is used properly, 
  that every method has a return statement, and so on.

  ###############################################################
  
20. Benefits from CLR?

Because Windows processes require a lot of operating system resources, having many of them
can hurt performance and limit available resources. 
Reducing the number of processes by running multiple applications in a single operating system process 
can improve performance, require fewer resources, 
and be just as robust as if each application had its own process. 
This is another benefit of
managed code as compared to unmanaged code.

###############################################################

21. What is "The Common Type System".

###############################################################

22. What are two kinds of assemblies?

Strongly assembly type: 

strongly named assembly is signed with a publisher’s public/private key pair 
that uniquely identifies the assembly’s publisher.
This key pair allows the assembly to be uniquely identified, secured, and
versioned, and it allows the assembly to be deployed anywhere on the user’s machine or even on the
Internet.

However, we have a problem: Two (or more) companies could produce assemblies
that have the same file name. Then, if both of these assemblies get copied into the same well-known
directory, the last one installed wins, and all of the applications that were using the old assembly no
longer function as desired.

A strongly named assembly consists of four attributes that uniquely identify the assembly: 
a file name (without an extension), a version number, a culture identity, and a public key.

"MyTypes, Version=1.0.8123.0, Culture=neutral, PublicKeyToken=b77a5c561934e089"

###############################################################

23. How to create a strongly named assembly?

- is to obtain a key by using the Strong Name utility, SN.exe, 
  that ships with the .NET Framework SDK and Microsoft Visual Studio.
  
  Command "SN –k MyCompany.snk" generates a public/private key pair.

- generate a public key token. 
  A public key token is a 64-bit hash of the public key. 
  SN.exe’s –tp switch shows the public key token 
  that corresponds to the complete public key at the end of its output.
  
- Compile the assembly with this key.

###############################################################

24. What are benefits from strongly named assemblies?

Signing an assembly with a private key and embedding the signature and public key within an assembly
allows the CLR to verify that the assembly has not been modified or corrupted. When an
assembly is installed into the GAC, the system hashes the contents of the file containing the manifest
and compares the hash value with the RSA digital signature value embedded within the PE file (after
unsigning it with the public key).


25. How are new resources allocated?

- Initializing the managed heap.
  The CLR also maintains a pointer, which I’ll call NextObjPtr. 
  This pointer indicates where the next object is to be allocated within the heap. 
  Initially, NextObjPtr is set to the base address of the address space region.
  
- Calculate the number of bytes required for the type’s fields 
  (and all the fields it inherits from its base types).
  
- Allocates memory from NextObjPtr and zeros it 
  and after that moves a pointer further.
  
26. What is the Garbage collection algorithm?

When there are not enough free memory. CLR performs a Garbage Collection.
Reference counting. Each object increases reference count when it acquires it
and starts processing it. When the object is not longer needed it's reference
count is decremented.

When we have circular references this algorithm does not work.
Instead of it we use reference tracking algorithm.

From the start:
1. Suspend all threads in the process.
2. The marking phrase of GC.

First, it walks through all the objects in the heap
setting a bit (contained in the sync block index field) to 0. 
This indicates that all objects should be deleted. 
Then, the CLR looks at all active roots to see which objects they refer to. 
This is what makes the CLR’s GC a reference tracking GC. 
If a root contains null, the CLR ignores the root 
and moves on to examine the next root.


27. How does the GC treat cycle references?

1. Start.

It starts from active roots. 
If a root does not have descendants it is skipped.
Otherwise we loop through all descendants and mark them as undeleted.
Then we process all descendants of these descendants and skip those 
nodes that have already been marked. In this way we avoid an infinite loop.


Once complete, the heap contains some marked and some unmarked objects. 
The marked objects must survive the collection because 
there is at least one root that refers to the object;

All slots of memory which are marked with 0 will be deleted.

2. GC’s compacting phase.

During the compacting phase, the CLR shifts the memory consumed
by the marked objects down in the heap, compacting all the surviving objects 
together so that they are contiguous in memory.

Reducing your application’s working set size,
thereby improving the performance of accessing these objects in the future.

Second, the free space is all contiguous as well, 
so this region of address space can be freed, 
allowing other things to use it.

Because some objects have been shifted in memory to other addresses.
That's why their descendant objects now point 
to an invalid or corrupted addresses of memory.
We should loop through them and change addresses to correct ones. 

3. Change NextObjPtr to a new free memory position.
4. Resume all suspended threads.


28. What do generations mean?

Generational garbage collection makes the following assumptions:
- The newer an object is, the shorter its lifetime will be.
- The older an object is, the longer its lifetime will be.
- Collecting a portion of the heap is faster than collecting the whole heap.

Generation 0:

When initialized, the managed heap contains no objects. Objects added to the heap are said to be
in generation 0. Stated simply, objects in generation 0 are newly constructed objects that the garbage
collector has never examined.

 -------------------------------------------------
| A | B | C | D | E |                             |
 -------------------------------------------------
|-------------------> 
  Generation 0
  
When the CLR initializes, it selects a budget size (in kilobytes) for generation 0. So if allocating a
new object causes generation 0 to surpass its budget, a garbage collection must start. Let’s say that
objects A through E fill all of generation 0. When object F is allocated, a garbage collection must start.

Start garbage collection and scan all objects in memory.
And if turns out that A,B,D are still referenced and 
objects C,E are not already referenced and we can delete them.
Garbage collector deletes them and shifts existed objects to 
make them adjacent.

 -------------------------------------------------
| A | B | D |                         |           |
 -------------------------------------------------
|----------> ------------------------>
Generation 1     Generation 0
  
The objects that survived one garbage collection 
are marked as generation 1.

Than we allocate new objects to generation 1:


 -------------------------------------------------
| A | B | D |  F  | G | H | I | J | K |           |
 -------------------------------------------------
|----------> ------------------------>
Generation 1     Generation 0

B,H,J  became unreachable and need to be reclaimed

If generation 0 exceeds its budget we need to run a garbage 
collection and we take only objects from generation 0.
We skip generation 1.

The first assumption is that newly created objects have a short lifetime.
So generation 0 is likely to have a lot of garbage in it, 
and collecting generation 0 will therefore reclaim a lot of memory.

If a root or an object refers to an object in an old generation, the garbage
collector can ignore any of the older objects’ inner references, decreasing the amount of time
required to build the graph of reachable objects.

29. What does finalization mean?

Finalization allows an object to execute some code after 
the object has been determined to be garbage but before
the object’s memory is reclaimed from the managed heap.
 
System.Object, the base class of everything, defines a protected 
and virtual method called Finalize.
When the garbage collector determines that an object is garbage, 
it calls the object’s Finalize method (if it is overridden).

Furthermore, be aware of the fact that you have 
no control over when the Finalize method will execute.
Finalize methods run when a garbage collection occurs, 
which may happen when your application requests more memory.

30. What is GC.SuppressFinalize()?

SuppresFinalize should only be called by a class that has a finalizer. 
It's informing the GC that this object was cleaned up fully.
The recommended IDisposable pattern when you have a finalizer is....

public class MyClass : IDisposable
{
    private bool disposed = false;

    protected virtual void Dispose(bool disposing)
    {
        if (!disposed)
        {
            if (disposing)
            {
                // called via myClass.Dispose(). 
                // OK to use any private object references
            }

            disposed = true;
        }
    }

    public void Dispose() // Implement IDisposable
    {
        Dispose(true);
        GC.SuppressFinalize(this);
    }

    ~MyClass() // the finalizer
    {
        Dispose(false);
    }
}

31. How does IDisposable work?

// Create the temporary file.
using (FileStream fs = new FileStream("Temp.dat", FileMode.Create)) 
{
   // Write the bytes to the temporary file.
   fs.Write(bytesToWrite, 0, bytesToWrite.Length);
}

Then you access the variable via code contained inside using’s braces. 
When you compile this code, the compiler automatically emits the try and finally blocks. 
Inside the finally block, the compiler emits code to cast the object 
to an IDisposable and calls the Dispose method.

// Create the temporary file.
FileStream fs = new FileStream("Temp.dat", FileMode.Create);
try 
{
   // Write the bytes to the temporary file.
   fs.Write(bytesToWrite, 0, bytesToWrite.Length);
}
finally 
{
   // Explicitly close the file when finished writing to it.
   if (fs != null) fs.Dispose();
}

FileStream   fs = new FileStream("DataFile.dat", FileMode.Create);
StreamWriter sw = new StreamWriter(fs);
sw.Write("Hi there");
// The following call to Dispose is what you should do.
sw.Dispose();
// NOTE: StreamWriter.Dispose closes the FileStream;
// the FileStream doesn't have to be explicitly closed.

What do you think would happen if there were no code to explicitly call Dispose? 

Well, at some point, the garbage collector would correctly detect 
that the objects were garbage and finalize them. 
But the garbage collector doesn’t guarantee the order 
in which objects are finalized. 
So if the FileStream object were finalized first, it would close the file. 
Then when the StreamWriter object was finalized, 
it would attempt to write data to the closed file, throwing an exception. 
If, on the other hand, the StreamWriter object were finalized first, 
the data would be safely written to the file.


32. How does finalization work?

- Application creates a new object.
  The new operator allocates the memory from the heap.
  If the object’s type defines a Finalize method, 
  a pointer to the object is placed on the finalization list  
  
- Scan. When the garbage collector finds an object and 
  finds it in the finalization list it adds it to 
  F-reachable queue.
  
- A special high-priority CLR thread is dedicated to calling Finalize methods. 
  A dedicated thread is used to avoid potential thread synchronization situations 
  that could arise if one of the application’s normal-priority threads were used instead.
  When the freachable queue is empty (the usual case), this
  thread sleeps. But when entries appear, this thread wakes, removes each entry from the queue, and
  then calls each object’s Finalize method.
  When the freachable queue is empty (the usual case), this thread sleeps. 
  But when entries appear, this thread wakes, removes each entry from the queue, 
  and then calls each object’s Finalize method.
  
  The garbage collector compacts the reclaimable memory, 
  which promotes the resurrected object to an older generation (not ideal). 
  And now, the special finalization thread empties the freachable queue, 
  executing each object’s Finalize method.

  The important point to get from all of this is 
  that two garbage collections are required to reclaim memory 
  used by objects that require finalization.
  
33. How does weak references work?

  When a root points to an object, the object cannot be collected 
  because the application's code can reach the object. 
  When a root points to an object, it's called a strong reference to the object. 
  However, the garbage collector also supports weak references. 
  Weak references allow the garbage collector to collect the object, 
  but they also allow the application to access the object.
  
  Void Method() 
  {
      // Creates a strong reference to the
      // object.
      Object o = new Object();    

      // Create a strong reference to a short WeakReference object.
      // The WeakReference object tracks the Object.
      WeakReference wr = new WeakReference(o);

      o = null;    // Remove the strong reference to the object

      o = wr.Target;
      if (o == null) 
	  {
          // A GC occurred and Object was reclaimed.
      } 
	  else 
	  {
          // a GC did not occur and we can successfully access the Object 
          // using o
      }
  }

  When the user switches away from the first part of the application, 
  you can create a weak reference to the tree and destroy all strong references. 
  If the memory load is low for the other part of the application, 
  then the garbage collector will not reclaim the tree's objects. 
  
  When the user switches back to the first part of the application, 
  the application attempts to obtain a strong reference for the tree. 
  If successful, the application doesn't have to traverse the user's hard drive again.
  
  WeakReference(Object target, Boolean trackResurrection); 
  
  The trackResurrection parameter indicates whether the WeakReference object 
  should track the object after it has had its Finalize method called.
  
  How to use weak references?
  For example if we have read a very large object into memory, object 
  that contains information about all drives, folders and files.
  We obtain a strong reference:
  
  Object obj = new Object(); 
  
  Then we use it for some small period of time and switch over
  to another window. And we do not want to waste memory.
  So that we transform our strong reference to weak reference:
  
  WeakReference wr = new WeakReference(obj);
  
  It means that this object is available for garbage collection
  and if garbage collection occurs it will reclaim this memory.
  But if the application uses memory seldom so that garbage collection
  occurs rarely as well and this memory won't be deleted with large possibility.
  But if switch back to the original window, 
  we should return a reference to the tree.
  
  obj = wr.Targtet;
  
  if wr.Target is null, it means that garbage collection has already occured.
  
  if (wr.Target == null)
     we should load the object from the file system.
   
  The managed heap contains two internal data structures 
  whose sole purpose is to manage weak references: 
  the short weak reference table and the long weak reference table. 
  These two tables simply contain pointers to objects allocated within the managed heap. 
  
  Weak references inside:
  
  We switch over to another window and suppose we have run out whole memory.
  Garbage collection is started:
  1. It builds a graph of reachable objects.
  2. It scans short weak reference table and finds one object.
     If this object is not part of the graph it is considered unreachable
	 and will be freed and then sets the weak reference to null.
  
 
34. Examples of memory leak in .NET?

- Memory leak with delegates:

private delegate void MessageWriter(string text);

public class ConsoleWriter
{
   public void WriteToConsole(string text)
   {
      Console.Write(text);
   }
}

// Client code...
ConsoleWriter writerObj = new ConsoleWriter();
MessageWriter writerDel = new MessageWriter(writerObj.WriteToConsole);

writerDel("Hello, World!");
                                        ------------
 ------------     ------------------>  | writerObj  |
| writerDel  |   | strong reference     ------------
|------------|   | 
| target     |---                      -------------
| method     |----------------------> | writerDel   |
 ------------                          -------------
 
 For example if we have have a and do the event subscription here.
 
 public class EventListener
 {
     StateObject  _obj;
 
     public EventListener(StateObject obj)
	 {
	     _obj = obj;
         _obj.SomeEvent += new EventHandler();		 
	 }
	 
	 public void SomeMethod(object sender, EventArgs args)
	 {
	     // Do some work here 
	 }
 }
 
 public class StateObject
 {
     public event SomeEvent;
 }
 
 
 public class Program
 {
     public void Main()
	 {
	     var state1 = new StateObject();
	     var event1 = new EventListener();
		 event1 = null;
		 
		 // Call garbage collection here.
		 // It should be deleted. But 
		 GC.Collect();
		 
		 // But after memory collection event1 should be deleted.
		 // But ii won't be deleted. state1 contains reference to it.
	 }
 }
 
 We can solve it by applying weak references.
                  
- Memory leak with static events.
				  
static event EventHandler Evil;

for(int i = 0 ; i < 1000000 ; i++)
    Evil += delegate {};

	
35. When do we need to create a new thread?

You need the thread to run with a non-normal thread priority. 
All thread pool threads run at normal priority. 
Although you can change this, it is not recommended, 
and the priority change does not persist across thread pool operations.

You want to start a thread and possibly abort 
it prematurely by calling Thread’s Abort method.

At this point, the thread is executing code 
and manipulating data in its process’s address space.
After another time-slice, Windows performs another context switch. 
Windows performs context switches from the moment the system 
is booted and continues until the system is shut down.

For example, if a priority 5 thread is running and the system determines 
that a higher-priority thread is ready to run, 
the system immediately suspends the lower-priority thread
(even if it’s in the middle of its time-slice) 
and assigns the CPU to the higher-priority thread, 
which gets a full time-slice.

36. What is CLR thread pool?

To improve
this situation, the CLR contains code to manage its own thread pool. You can think of a thread
pool as being a set of threads that are available for your application’s own use. There is one thread
pool per CLR; this thread pool is shared by all AppDomains controlled by that CLR.

When your application wants to perform an asynchronous operation,
you call some method that appends an entry into the thread pool’s queue.

The thread pool’s code will
extract entries from this queue and dispatch the entry to a thread pool thread.

However, when a thread pool thread has completed its task, the thread is
not destroyed; instead, the thread is returned to the thread pool, 
where it sits idle waiting to respond to another request.

If your application makes many requests of the thread pool, 
the thread pool will try to service all of the requests by using just this one thread. 
However, if your application is queuing up several requests faster 
than the thread pool thread can handle them, additional threads will be created.

//The method that queues a request to CLR's thread pool 
static Boolean QueueUserWorkItem(WaitCallback callBack);
static Boolean QueueUserWorkItem(WaitCallback callBack, Object state);

delegate void WaitCallback(Object state);

Example how to queue a request to the thread pool.


using System;
using System.Threading;

public static class Program 
{
    public static void Main() 
	{
        Console.WriteLine("Main thread: queuing an asynchronous operation");
		
        ThreadPool.QueueUserWorkItem(ComputeBoundOp, 5);
		
        Console.WriteLine("Main thread: Doing other work here...");
        Thread.Sleep(10000); // Simulating other work (10 seconds)
		
        Console.WriteLine("Hit <Enter> to end this program...");
        Console.ReadLine();
    }
	
    // This method's signature must match the WaitCallback delegate
    private static void ComputeBoundOp(Object state) 
	{
        // This method is executed by a thread pool thread
		
	    Console.WriteLine("In ComputeBoundOp: state={0}", state);
        Thread.Sleep(1000); // Simulates other work (1 second)
		
        // When this method returns, the thread goes back
        // to the pool and waits for another task
	}
}

37. What is execution context?

Some information which is passed to a child thread by a parent thread.
We should suppress this information to gain perfomance.

public static void Main() 
{
    // Put some data into the Main thread's logical call context
    CallContext.LogicalSetData("Name", "Jeffrey");
	
    // Initiate some work to be done by a thread pool thread
    // The thread pool thread can access the logical call context data
    ThreadPool.QueueUserWorkItem(state => Console.WriteLine("Name={0}", CallContext.LogicalGetData("Name")));
	
    // Now, suppress the flowing of the Main thread's execution context
    ExecutionContext.SuppressFlow();
	
	// Initiate some work to be done by a thread pool thread
    // The thread pool thread CANNOT access the logical call context data
    ThreadPool.QueueUserWorkItem(state => Console.WriteLine("Name={0}", CallContext.LogicalGetData("Name")));
	
    // Restore the flowing of the Main thread's execution context in case
    // it employs more thread pool threads in the future
    ExecutionContext.RestoreFlow();
	
    //...
    Console.ReadLine();
}


38. What is cooperative cancellation?

internal static class CancellationDemo 
{
    public static void Main() 
	{
        CancellationTokenSource cts = new CancellationTokenSource();
		
        // Pass the CancellationToken and the number-to-count-to into the operation
        ThreadPool.QueueUserWorkItem(o => Count(cts.Token, 1000));
		
        Console.WriteLine("Press <Enter> to cancel the operation.");
        Console.ReadLine();
		
		// If Count returned already, Cancel has no effect on it
        cts.Cancel();
		
        // Cancel returns immediately, and the method continues running here...
        Console.ReadLine();
	}
	
	private static void Count(
	    CancellationToken        token, 
		Int32                    countTo) 
    {
        for (Int32 count = 0; count <countTo; count++) 
		{
            if (token.IsCancellationRequested) 
			{
                Console.WriteLine("Count is cancelled");
                break; // Exit the loop to stop the operation
            }
			
            Console.WriteLine(count);
            Thread.Sleep(200);    // For demo, waste some time
        }
		
        Console.WriteLine("Count is done");
    }
}


39. How does task work?

ThreadPool.QueueUserWorkItem(ComputeBoundOp, 5); // Calling QueueUserWorkItem
new Task(ComputeBoundOp, 5).Start();             // Equivalent of preceding using Task
Task.Run(() => ComputeBoundOp(5));               // Another equivalent



40. How to wait for a task to finish execution?

private static Int32 Sum(Int32 n) 
{
   Int32 sum = 0;
   for (; n > 0; n--)
      checked { sum += n; } // if n is large, this will throw System.OverflowException
   return sum;
}

// Create a Task (it does not start running now)
Task<Int32> t = new Task<Int32>(n => Sum((Int32)n), 1000000000);

// You can start the task sometime later
t.Start();

// Optionally, you can explicitly wait for the task to complete
t.Wait(); // FYI: Overloads exist accepting timeout/CancellationToken

// You can get the result (the Result property internally calls Wait)
Console.WriteLine("The Sum is: " + t.Result); // An Int32 value

When a thread calls the Wait method, the system checks 
if the Task that the thread is waiting for has started executing. 

If it has, then the thread calling Wait will block
until the Task has completed running. 

But if the Task has not started executing yet, then
the system may (depending on the TaskScheduler) execute the Task by using the thread
that called Wait. 

If this happens, then the thread calling Wait does not block; it executes
the Task and returns immediately. This is good in that no thread has blocked, thereby
reducing resource usage (by not creating a thread to replace the blocked thread) while
improving performance (no time is spent to create a thread and there is no context switching).
But it can also be bad if, for example, the thread has taken a thread synchronization
lock before calling Wait and then the Task tries to take the same lock, resulting in a deadlocked
thread!


41. What happens if an exception has been thrown?

If the compute-bound task throws an unhandled exception, the exception will be swallowed,
stored in a collection, and the thread pool thread is allowed to return to the thread pool.
When the Wait method or the Result property is invoked, 
these members will throw a System.AggregateException object.

Wait for all threads to complete.

In addition to waiting for a single task, 
the Task class also offers two static methods that allow a
thread to wait on an array of Task objects. 
Task’s static WaitAny method blocks the calling thread
until any of the Task objects in the array have completed.

Similarly, the Task class has a static WaitAll method 
that blocks the calling thread until all the Task objects in the array have completed. 
The WaitAll method returns true if all the Task objects complete and false if a timeout occurs;

42. How to start a new task automatically when another completes?

Calling  task.Result causes the thread to wait until the task completes.
It can hurt perfomance. So to know when task completes we can use ContinueWith

// Create and start a Task, continue with another task
Task<Int32> t = Task.Run(() => Sum(CancellationToken.None, 10000));

// ContinueWith returns a Task but you usually don't care
Task cwt = t.ContinueWith(task => Console.WriteLine("The sum is: " + task.Result));

When an exception has occured in task we should process it:
- We can do it by applying ContinueWith method:

var task = Task.Factory.StartNew(() =>
{
    // Throws an exception 
    // (possibly from within another task spawned from within this task)
});

var failureTask = task.ContinueWith((t) =>
{
    // Flatten and loop (since there could have been multiple tasks)
    foreach (var ex in t.Exception.Flatten().InnerExceptions)
        Console.WriteLine(ex.Message);
}, TaskContinuationOptions.OnlyOnFaulted);

- We can specify ContinueWith and check for an exception there.

var task = Task.Factory.StartNew(() =>
{
    // Throws an exception 
    // (possibly from within another task spawned from within this task)
});

task.ContinueWith(task =>
{
    if (task.IsFaulted)
    {	
	    // handle errors
	}
	else 
	{
	    Console.WriteLine(task.Result.Headers);
    }
});

43. How to use child tasks?

Create a parent task that contains 3 child tasks

Task<Int32[]> parent = new Task<Int32[]>(() => 
{
    // Create an array for the results
    var results = new Int32[3]; 
	
    // This tasks creates and starts 3 child tasks
	var task1 = new Task(() => results[0] = Sum(10000), TaskCreationOptions.AttachedToParent);
	task1.Start();
    
	var task2 = new Task(() => results[1] = Sum(20000), TaskCreationOptions.AttachedToParent);
    task2.Start();
	
	var task3 = new Task(() => results[2] = Sum(30000), TaskCreationOptions.AttachedToParent);
	task3.Start();
	
    // Returns a reference to the array (even though the elements may not be initialized yet)
    return results;
});

Create a task that will be executed after the parent and all its child tasks.

// When the parent and its children have run to completion, display the results
var cwt = parent.ContinueWith(parentTask => Array.ForEach(parentTask.Result, Console.WriteLine));

// Start the parent Task so it can start its children
parent.Start();


44. What is TaskSchedulers?

A task scheduler makes sure that the work of a task is eventually executed. 
The default task scheduler is based on the .NET Framework 4 ThreadPool, 
which provides work-stealing for load-balancing, 
thread injection/retirement for maximum throughput, 
and overall good performance.


// Provides a task scheduler that ensures a maximum concurrency level while  
// running on top of the thread pool. 
public class LimitedConcurrencyLevelTaskScheduler : TaskScheduler
{
   // The list of tasks to be executed  
   private readonly LinkedList<Task> _tasks = new LinkedList<Task>(); // protected by lock(_tasks) 
   
   protected sealed override void QueueTask(Task task)
   {
       lock (_tasks)
       {
           _tasks.AddLast(task);
		   //notify thread pool to process thease threads
       }		   
   }
   
   protected sealed override bool TryExecuteTaskInline(Task task, bool taskWasPreviouslyQueued)
   {
       
   }
}

IOTaskScheduler This task scheduler queues tasks to the thread pool’s I/O threads instead
of its worker threads.

LimitedConcurrencyLevelTaskScheduler This task scheduler allows no more than n (a
constructor parameter) tasks to execute simultaneously.

OrderedTaskScheduler This task scheduler allows only one task to execute at a time. This
class is derived from LimitedConcurrencyLevelTaskScheduler and just passes 1 for n.


45. How to parallel foreach cycle?

- Paralleling of cycle 'for':

  // One thread performs all this work sequentially
  for (Int32 i = 0; i < 1000; i++) DoWork(i);
  
  can be paralleled as:
  
  // The thread pool’s threads process the work in parallel
  Parallel.For(0, 1000, i => DoWork(i));
  
- Parallelizing of cycle 'foreach':

  // One thread performs all this work sequentially
  foreach (var item in collection) DoWork(item);

  // The thread pool's threads process the work in parallel
  Parallel.ForEach(collection, item => DoWork(item));

- Methods:

  // One thread executes all the methods sequentially
  Method1();
  Method2();
  Method3();
  
  // The thread pool’s threads execute the methods in parallel
  Parallel.Invoke(
    () => Method1(),
    () => Method2(),
    () => Method3());

	
46. How does the thread pool manager manage its threads?


The ThreadPool.QueueUserWorkItem method 
and the Timer class always queue work items to the global queue. 
Worker threads pull items from this queue 
using a first-in-first-out (FIFO) algorithm and process them.

Because multiple worker threads can be removing items from the global
queue simultaneously, all worker threads contend on a thread synchronization lock to ensure that
two or more threads don’t take the same work item.

The CLR Thread pool consists of GlobalQueue and Worker Threads 
And each worker thread has its own local queue.

Non-Worker Thread posts a task into Global Queue.
TaskScheduler schedules a task and task is added to the global queue.
But, each worker thread has its own local queue, and when a
worker thread schedules a Task, the Task is added to calling the thread’s local queue.

When a worker thread is ready to process an item, 
it always checks its local queue for a Task first.
If a Task exists, the worker thread removes 
the Task from its local queue and processes the item.

Because a worker thread is the only thread allowed 
to access the head of its own local queue, 
no thread synchronization lock is required and adding 
and removing Tasks from the queue is very fast.

Thread Pool consists of two pools:

1) The Worker ThreadPool.
Designed to provide services at the level of CPU parallelism, 
the worker ThreadPool takes advantage of multi-core architectures. 
For the former, the ThreadPool implementation makes use of strategies 
such as lock-free queues to avoid contention and work-stealing for load balancing

2) I/O ThreadPool.
This part of the ThreadPool implementation, 
related to I/O parallelism, handles blocking workloads 
(that is, I/O requests that take a relatively long time to service) or asynchronous I/O. 
In asynchronous calls, threads aren’t blocked 
and can continue to do other work while the request is serviced. 



47. What is work-stealing tecniques?

The library has a task manager that, by default, uses one worker thread per processor. 
This ensures minimal thread switching by the OS.
Each worker thread has its own local task queue of work to be done. 
Each worker usually just pushes new tasks onto its own queue 
and pops work whenever a task is done. 
When its local queue is empty, a worker looks for work itself 
and tries to "steal" work from the queues of other workers. 


48. How does I/O ThreadPool work?

ReadFile then calls into the Windows kernel 
by having your thread transition from native/usermode
code to native/kernel-mode code, 
passing the IRP data structure to the kernel.
Windows delivers the IRP to the appropriate device driver’s IRP queue.
Each device driver maintains its own IRP queue 
that contains I/O requests from all processes running on the machine.

But here is the important part: 
While the hardware device is performing the I/O operation, 
your thread that issued the I/O request has nothing to do, 
so Windows puts your thread to sleep so that it is not wasting CPU time (#6). 

This is great, but although your thread is not wasting time, 
it is wasting space (memory), as its user-mode stack, 
kernel-mode stack, thread environment block (TEB), 
and other data structures are sitting in memory but are not being accessed at all. 
In addition, for GUI applications, 
the UI can’t respond to user input while the thread is blocked. 
All of this is bad.


49. How does Thread Pool perform a syncronous IO operation?

Client makes a query to CLR thread pool. Then the query goes to
CLR global queue. Then someone from worker threads picks it up
from the global queue and puts it into its own.
Then the worker thread performs the syncronous operation. 
Then it calls FileStream Read method. 
It sends a request to a file system driver. The request is 
a file handler and a position inside file, number of bytes
to be read 
Then the thread is blocked. When a thread is blocked it 
does not use CPU but it is still memory.
The request goes to the device's queue.
The thread is put to sleep state.

The thread waits until the file driver completes the request.
when it completes it the thread wakes up and continues 
processing. The problem is that there can be a lot of waken thread
which at the same moment start processing and CPU must do 
context switching.

In case is an asyncrinous operation is pushed the thead continues 
working after posting a request to the file system queue.
If the request have been processed CLR posts another request to
the thread pool and some worker thread will complete the request.
We can avoid context switching.

50. Example of asyncronous functions?

private static async Task<String> IssueClientRequestAsync(String serverName, String message) 
{
    using (var pipe = new NamedPipeClientStream(
	                        serverName, 
	                        "PipeName", 
							PipeDirection.InOut,
                            PipeOptions.Asynchronous | PipeOptions.WriteThrough)) 
	{
	    // Must Connect before setting ReadMode
        pipe.Connect();   
        pipe.ReadMode = PipeTransmissionMode.Message;
		
        // Asynchronously send data to the server
        Byte[] request = Encoding.UTF8.GetBytes(message);
        await pipe.WriteAsync(request, 0, request.Length);
		
		// Asynchronously read the server's response
        Byte[] response = new Byte[1000];
        Int32 bytesRead = await pipe.ReadAsync(response, 0, response.Length);
        return Encoding.UTF8.GetString(response, 0, bytesRead);
	}
}

When you mark a method as async, 
the compiler basically transforms your method’s code 
into a type that implements a state machine 
(the details of which will be discussed in the next section).

The reason is because one thread might execute
the code before the await and a different thread 
might execute the code after the await. 
If you use await within a C# lock statement, 
the compiler issues an error.

internal sealed class Type1 
{
 
}

internal sealed class Type2 
{

}

private static async Task<Type1> Method1Async() 
{
    /* Does some async thing that returns a Type1 object */
}

private static async Task<Type2> Method2Async() 
{
    /* Does some async thing that returns a Type2 object */
}

private static async Task<String> MyMethodAsync(Int32 argument) 
{
    Int32 local = argument;
	
    try 
	{
        Type1 result1 = await Method1Async();
		
        for (Int32 x = 0; x < 3; x++) 
		{
            Type2 result2 = await Method2Async();
        }
    }
    catch (Exception) 
	{
        Console.WriteLine("Catch");
    }
    finally 
	{
        Console.WriteLine("Finally");
    }
	
    return "Done";
}

When compiling MyMethodAsync, the compiler transforms the code 
in this method to a state machine structure that is
capable of being suspended and resumed.


// AsyncStateMachine attribute indicates an async method (good for tools using reflection);
// the type indicates which structure implements the state machine
[DebuggerStepThrough, AsyncStateMachine(typeof(StateMachine))]
private static Task<String> MyMethodAsync(Int32 argument) 
{
    // Create state machine instance & initialize it
    StateMachine stateMachine = new StateMachine() 
	{
       // Create builder returning Task<String> from this stub method
       // State machine accesses builder to set Task completion/exception
	   
       m_builder  = AsyncTaskMethodBuilder<String>.Create(),
       m_state    = -1,                   // Initialize state machine location
       m_argument = argument              // Copy arguments to state machine fields
    };

    // Start executing the state machine
    stateMachine.m_builder.Start(ref stateMachine);
    return stateMachine.m_builder.Task;   // Return state machine's Task
}

// This is the state machine structure
[CompilerGenerated, StructLayout(LayoutKind.Auto)]
private struct StateMachine : IAsyncStateMachine 
{
    // Fields for state machine's builder (Task) & its location
    public   AsyncTaskMethodBuilder<String>  m_builder;
    public   Int32                           m_state;
	
    // Argument and local variables are fields now:
    public   Int32    m_argument, m_local, m_x;
    public   Type1    m_resultType1;
    public   Type2    m_resultType2;
	
    // There is 1 field per awaiter type.
    // Only 1 of these fields is important at any time. That field refers
    // to the most recently executed await that is completing asynchronously:
    private  TaskAwaiter<Type1> m_awaiterType1;
    private  TaskAwaiter<Type2> m_awaiterType2; 
	
    // This is the state machine method itself
    void IAsyncStateMachine.MoveNext() 
	{
        String result = null;     // Task's result value
		
        // Compiler-inserted try block ensures the state machine’s task completes
        try 
		{
            Boolean executeFinally = true;        // Assume we're logically leaving the 'try' block
			
            if (m_state == -1) 
			{ 
			    // If 1st time in state machine method, 
                m_local = m_argument;             // execute start of original method
            }

            // Try block that we had in our original code
            try 
			{
                TaskAwaiter<Type1> awaiterType1;
                TaskAwaiter<Type2> awaiterType2;
				
                switch (m_state) 
				{
                    case -1: 
					    // Start execution of code in 'try'
                        // Call Method1Async and get its awaiter
                        awaiterType1 = Method1Async().GetAwaiter();
                        if (!awaiterType1.IsCompleted) 
						{
                            m_state = 0;                     // 'Method1Async' is completing
							
                            // asynchronously
                            m_awaiterType1 = awaiterType1;   // Save the awaiter for when we come back
							
                            // Tell awaiter to call MoveNext when operation completes
                            m_builder.AwaitUnsafeOnCompleted(ref awaiterType1, ref this);
							
                            // The line above invokes awaiterType1's OnCompleted which approximately
                            // calls ContinueWith(t => MoveNext()) on the Task being awaited.
                            // When the Task completes, the ContinueWith task calls MoveNext
                            executeFinally = false;          // We're not logically leaving the 'try'
							
                            // block
                            return; // Thread returns to caller
                        }
                        // 'Method1Async' completed synchronously
                        break; 				
						
					case 0: 
					    // 'Method1Async' completed asynchronously
                        awaiterType1 = m_awaiterType1;     // Restore most-recent awaiter
                        break;
						
					case 1: 
					    // 'Method2Async' completed asynchronously
                        awaiterType2 = m_awaiterType2; // Restore most-recent awaiter 
                        goto ForLoopEpilog;
				}
				
				// After the first await, we capture the result & start the 'for' loop
				
                m_resultType1 = awaiterType1.GetResult(); // Get awaiter's result
                ForLoopPrologue:
                    m_x = 0;           // 'for' loop initialization
                    goto ForLoopBody;  // Skip to 'for' loop body
				
				ForLoopEpilog:
                    m_resultType2 = awaiterType2.GetResult();
                    m_x++;             // Increment x after each loop iteration
                    // Fall into the 'for' loop’s body
					
				ForLoopBody:
                    if (m_x < 3) 
					{ 
					    // 'for' loop test
                        // Call Method2Async and get its awaiter
						
                        awaiterType2 = Method2Async().GetAwaiter();
                        if (!awaiterType2.IsCompleted) 
						{
                            m_state        = 1;             // 'Method2Async' is completing asynchronously
                            m_awaiterType2 = awaiterType2;  // Save the awaiter for when we come back
							
                            // Tell awaiter to call MoveNext when operation completes
                            m_builder.AwaitUnsafeOnCompleted(ref awaiterType2, ref this);
                            executeFinally = false;         // We're not logically leaving the 'try' block
                            return;                         // Thread returns to caller
                        }
						
                        // 'Method2Async' completed synchronously
                        goto ForLoopEpilog; // Completed synchronously, loop around
                    }
			}
			catch (Exception) 
			{
                Console.WriteLine("Catch");
            }
            finally 
			{
                // Whenever a thread physically leaves a 'try', the 'finally' executes
                // We only want to execute this code when the thread logically leaves the 'try'
                if (executeFinally) 
				{
                    Console.WriteLine("Finally");
                }
            }
			
            result = "Done"; // What we ultimately want to return from the async function
        }
	}
}


51. How can thread synchronization hurt perfomance?

When many threads simultaneously try to acquire a lock 
only one of them  can get it. Others will block until it processes
a request. So the thread pool will create more threads to keep itself saturated.

52. Are static methods thread safe?  Yes.

53. Examples of a thread-safe static method.

public static Int32 Max(Int32 val1, Int32 val2) 
{
    return (val1 < val2) ? val2 : val1;
}

This function is thread-safe because we work with Value types 
and each thread gets a copy.

54. Kernel-mode syncronysation primitives.

This brings us to the primitive kernel-mode constructs. 
The kernel-mode constructs are provided by the Windows operating system itself. 
As such, they require that your application’s threads 
call functions implemented in the operating system kernel. 

Having threads transition from user mode to kernel mode and back incurs a big performance hit, 
which is why kernel-mode constructs should be avoided.

However, they do have a positive feature—
when a thread uses a kernel-mode construct to
acquire a resource that another thread has, 
Windows blocks the thread so that it is no longer wasting CPU time. 
Then, when the resource becomes available, 
Windows resumes the thread, allowing it to access the resource.

If the construct is a user-mode construct, 
the thread is running on a CPU forever, and we call this a livelock. 
If the construct is a kernel-mode construct, 
the thread is blocked forever, and we call this a deadlock.

In an ideal world, we’d like to have constructs that take the best of both worlds. 
That is, we’d like a construct that is fast and non-blocking 
(like the user-mode constructs) 
when there is no contention.
But when there is contention for the construct, we’d like it to be blocked by the operating system
kernel. Constructs that work like this do exist; I call them hybrid constructs


55. User-mode constructs.

Volatile Constructs.

internal static class StrangeBehavior 
{
    // As you'll see later, mark this field as volatile to fix the problem
    private static Boolean s_stopWorker = false;
	
    public static void Main() 
	{
        Console.WriteLine("Main: letting worker run for 5 seconds");
        Thread t = new Thread(Worker);
        t.Start();
		
        Thread.Sleep(5000);
        s_stopWorker = true;
		
        Console.WriteLine("Main: waiting for worker to stop");
        t.Join();
    }
	
	private static void Worker(Object o) 
	{
        Int32 x = 0;
        while (!s_stopWorker) 
		    x++;
			
        Console.WriteLine("Worker: stopped when x={0}", x);
    }
}

Well, the program has a potential problem due to all the optimizations that could happen to it. 
You see, when the Worker method is compiled, 
the compiler sees that s_stopWorker is either true or false, 
and it also sees that this value never changes inside the Worker method itself. 

So the compiler could produce code that checks s_stopWorker first. 
If s_stopWorker is true, then Worker: stopped when x=0 will be displayed. 

If s_stopWorker is false, then the compiler produces code 
that enters an infinite loop that increments x forever. 
You see, the optimizations cause the loop to run very fast 
because checking s_stopWorker only occurs once before the loop; 
it does not get checked with each iteration of the loop.


Another example:

internal sealed class ThreadsSharingData 
{
    private Int32 m_flag = 0;
    private Int32 m_value = 0;
	
    // This method is executed by one thread
    public void Thread1() 
	{
        // Note: These could execute in reverse order
        m_value = 5;
        m_flag = 1;
    }
	
    // This method is executed by another thread
    public void Thread2() 
	{
        // Note: m_value could be read before m_flag
        if (m_flag == 1)
            Console.WriteLine(m_value);	
	}
}

Volatile.Write 

The Volatile.Write method forces the value in location 
to be written to at the point of the call. 
In addition, any earlier program-order loads 
and stores must occur before the call to Volatile.Write.


The Volatile.Read method forces the value in location to be read from at the point of
the call. In addition, any later program-order loads and stores must occur after the call to
Volatile.Read.

internal sealed class ThreadsSharingData 
{
    private Int32 m_flag = 0;
    private Int32 m_value = 0;
	
    // This method is executed by one thread
    public void Thread1() 
	{
        // Note: 5 must be written to m_value before 1 is written to m_flag
        m_value = 5;
        Volatile.Write(ref m_flag, 1);
    }
	
    // This method is executed by another thread
    public void Thread2() 
	{
        // Note: m_value must be read after m_flag is read
        if (Volatile.Read(ref m_flag) == 1)
            Console.WriteLine(m_value);
    }
}


Interlocked Constructs.


Each of the methods in the Interlocked class performs an atomic read and write operation. 
In addition, all the Interlocked methods are full memory fences. 
That is, any variable writes before the call to an Interlocked method 
execute before the Interlocked method, and any variable reads
after the call execute after the call.

The Interlocked family of functions perform atomic operations on memory. 
How do they do it?

It depends on the underlying CPU architecture. 
For some CPUs, it's easy: 
The x86, for example, has direct support for many interlocked operations 
by means of the LOCK prefix 
(with the bonus feature that LOCK is implied for the XCHG opcode.) 
The ia64 and x64 also have direct support for atomic load-modify-store operations.

Most other architectures break the operation 
into two parts, known as Load-link/store-conditional. 
The first part (load-link) reads a value from memory 
and instructions the processor to monitor the memory address to see 
if any other processors modify that same memory. 

The second part (store-conditional) stores a value to memory 
provided that no other processors have written to the memory in the meantime. 
An atomic load-modify-store operation is therefore performed 
by reading the value via load-link, 
performing the desired computation, then attempting a store-conditional. 
If the store-conditional fails, then start all over again.


LONG InterlockedIncrement(LONG volatile *value)
{
  LONG lOriginal, lNewValue;
  do {
    // Read the current value via load-link so we will know if
    // somebody has modified it while we weren't looking.
    lOriginal = load_link(value);

    // Calculate the new value
    lNewValue = lOriginal + 1;

    // Store the value conditionally. This will fail if somebody
    // has updated the value in the meantime.
  } while (!store_conditional(value, lNewValue));
  return lNewValue;
}


56. What is Spin Lock?

internal struct SimpleSpinLock 
{
    private Int32 m_ResourceInUse;     // 0=false (default), 1=true
	
    public void Enter() 
	{
        while (true) 
		{
            // Always set resource to in-use
            // When this thread changes it from not in-use, return
			
            if (Interlocked.Exchange(ref m_ResourceInUse, 1) == 0) 
			    return;
				
            // Black magic goes here...
        }
    }
	
    public void Leave() 
	{
        // Set resource to not in-use
        Volatile.Write(ref m_ResourceInUse, 0);
    }
}

57. How can we implement another interlocked functions?
    For example Interlocked.Maximum?

public static Int32 Maximum(ref Int32 target, Int32 value) 
{
    Int32 currentVal = target, 
	      startVal, 
		  desiredVal;
		  
    // Don't access target in the loop except in an attempt
    // to change it because another thread may be touching it
    do 
	{
        // Record this iteration's starting value
        startVal = currentVal;
		
        // Calculate the desired value in terms of startVal and value
        desiredVal = Math.Max(startVal, value);
		
        // NOTE: the thread could be preempted here!
        // if (target == startVal) target = desiredVal
        // Value prior to potential change is returned
        currentVal = Interlocked.CompareExchange(ref target, desiredVal, startVal);
		
        // If the starting value changed during this iteration, repeat
    } 
	while (startVal != currentVal);
	
    // Return the maximum value when this thread tried to set it
    return desiredVal;
}


public static int CompareExchange(
	ref int location1,
	int value,
	int comparand
)

if (location1 == comparand)
{
    location1 = value;
    return location1;	
}    
else
    return location1;

	
How does it work?

1. 
   currentVal = target;

cycle:

   startVal   = currentVal;
   desiredVal = max(startVal, value);
   
   if (target == startVal)
   {
       currentVal = targer;
       target     = desiredVal;
   }
   else
   {
       currentVal = targer; 
	   continue;
   }
   
   
58. What are kernel-mode constructs?

Also, each method call on a kernel object causes the
calling thread to transition from managed code to native user-mode code to native kernel-mode
code and then return all the way back.
 
Benefits from kernel-mode constructs:

- When a kernel-mode construct detects contention on a resource, 
  Windows blocks the losing thread so that it is not spinning on a CPU, 
  wasting processor resources.

- Kernel-mode constructs can synchronize native and managed threads with each other.

- Kernel-mode constructs can synchronize threads running 
  in different processes on the same machine.

  
59. Single instance application example?

public static class Program 
{
   public static void Main() 
   {
       Boolean createdNew;
	  
       // Try to create a kernel object with the specified name
       using (new Semaphore(0, 1, "SomeUniqueStringIdentifyingMyApp", out createdNew)) 
	   {
           if (createdNew) 
		   {
               // This thread created the kernel object so no other instance of this
               // application must be running. Run the rest of the application here...
           } 
		   else 
		   {
               // This thread opened an existing kernel object with the same string name;
               // another instance of this application must be running now.
               // There is nothing to do in here, let's just return from Main to terminate
               // this second instance of the application.
           }
       }
   }
}

60. What are events?

Events are simply Boolean variables maintained by the kernel. 
A thread waiting on an event blocks 
when the event is false and unblocks when the event is true. 
There are two kinds of events. 
When an auto-reset event is true, 
it wakes up just one blocked thread, 
because the kernel automatically resets the event back to false 
after unblocking the first thread. 
When a manual-reset event is true,
it unblocks all threads waiting for it 
because the kernel does not automatically reset the event back to false; 
your code must manually reset the event back to false. 
The classes related to events look like this.


Using an auto-reset event, we can easily create a thread synchronization lock whose behavior is
similar to the SimpleSpinLock class I showed earlier.

internal sealed class SimpleWaitLock : IDisposable 
{
    private readonly AutoResetEvent m_available;
	
    public SimpleWaitLock() 
	{
        m_available = new AutoResetEvent(true); // Initially free
    }
	
    public void Enter() 
	{
        // Block in kernel until resource available
        m_available.WaitOne();
    }
	
    public void Leave() 
	{
        // Let another thread access the resource
        m_available.Set();
    }
	
    public void Dispose() { m_available.Dispose(); }
}

61. What is semafore

AutoReset Event 

If there are threads that have called 
waitHandle.WaitOne()
They are blocked until another main thread calls   
waitHandle.Set()


class BasicWaitHandle
{
  static EventWaitHandle _waitHandle = new AutoResetEvent (false);
 
  static void Main()
  {
    new Thread (Waiter).Start();
    Thread.Sleep (1000);                  // Pause for a second...
    _waitHandle.Set();                    // Wake up the Waiter.
  }
 
  static void Waiter()
  {
    Console.WriteLine ("Waiting...");
    _waitHandle.WaitOne();                // Wait for notification
    Console.WriteLine ("Notified");
  }
}


Two way signalling: When one thread 1 waits for thread 2 
to complete work then do its work and thread 2 waits for
thread 1 to complete its work.


class TwoWaySignaling
{
  static EventWaitHandle _ready = new AutoResetEvent (false);
  static EventWaitHandle _go = new AutoResetEvent (false);
  static readonly object _locker = new object();
  static string _message;
 
  static void Main()
  {
    new Thread (Work).Start();
 
    _ready.WaitOne();                  // First wait until worker is ready
    lock (_locker) _message = "ooo";
    _go.Set();                         // Tell worker to go
 
    _ready.WaitOne();
    lock (_locker) _message = "ahhh";  // Give the worker another message
    _go.Set();
    _ready.WaitOne();
    lock (_locker) _message = null;    // Signal the worker to exit
    _go.Set();
  }
 
  static void Work()
  {
    while (true)
    {
      _ready.Set();                          // Indicate that we're ready
      _go.WaitOne();                         // Wait to be kicked off...
      lock (_locker)
      {
        if (_message == null) return;        // Gracefully exit
        Console.WriteLine (_message);
      }
    }
  }
}

Manual reset event?

A ManualResetEvent functions like an ordinary gate. 
Calling Set opens the gate, allowing any number 
of threads calling WaitOne to be let through. 
Calling Reset closes the gate. 
Threads that call WaitOne on a closed gate will block; 
when the gate is next opened, they will be released all at once. 
Apart from these differences, a ManualResetEvent functions like an AutoResetEvent.

Set   switches the event over to sgnalling state.
Resrt makes it unsignalling. 

When we have one thread and it is performing some work
other threads are waiting for it. And when it finishes it
it wants to notify other threads at once

Semaphores can be useful in limiting concurrency 
— preventing too many threads from executing a particular piece of code at once. 
In the following example, five threads try 
to enter a nightclub that allows only three threads in at once:

class TheClub      // No door lists!
{
  static SemaphoreSlim _sem = new SemaphoreSlim (3);    // Capacity of 3
 
  static void Main()
  {
    for (int i = 1; i <= 5; i++) new Thread (Enter).Start (i);
  }
 
  static void Enter (object id)
  {
    Console.WriteLine (id + " wants to enter");
    _sem.Wait();
    Console.WriteLine (id + " is in!");           // Only three threads
    Thread.Sleep (1000 * (int) id);               // can be here at
    Console.WriteLine (id + " is leaving");       // a time.
    _sem.Release();
  }
}


When multiple threads are waiting on a semaphore, 
releasing the semaphore causes releaseCount threads to become unblocked 
(where releaseCount is the argument passed to Semaphore’s Release method).

Threads enter the semaphore by calling the WaitOne method, 
which is inherited from the WaitHandle class. 
When the call returns, the count on the semaphore is decremented. 
When a thread requests entry and the count is zero, the thread blocks. 
As threads release the semaphore by calling the Semaphore.
Release method, blocked threads are allowed to enter. 


61. What is mutex?

A Mutex represents a mutual-exclusive lock. 
It works similar to an AutoResetEvent or a Semaphore with a count 
of 1 because all three constructs release only one waiting thread at a time. 
The following shows what the Mutex class looks like.

Second, Mutex objects maintain a recursion count indicating 
how many times the owning thread owns the Mutex. 

If a thread currently owns a Mutex and then that thread waits on the Mutex again,
the recursion count is incremented and the thread is allowed to continue running. 
When that thread calls ReleaseMutex, the recursion count is decremented. 
Only when the recursion count becomes 0 can another thread become the owner of the Mutex.

internal class SomeClass : IDisposable 
{
    private readonly Mutex m_lock = new Mutex();
	
    public void Method1() 
	{
        m_lock.WaitOne();
		
        // Do whatever...
        Method2(); 
		
		// Method2 recursively acquires the lock
        m_lock.ReleaseMutex();
    }
	
    public void Method2() 
	{
        m_lock.WaitOne();
		
        // Do whatever...
        m_lock.ReleaseMutex();
    }
	
    public void Dispose() { m_lock.Dispose(); }
}

Generally you only use a Mutex across processes, e.g. 
if you have a resource that multiple applications must share, 
or if you want to build a single-instanced app 
(i.e. only allow 1 copy to be running at one time).

A semaphore allows you to limit access to a specific number of simultaneous threads, 
so that you could have, for example, 
a maximum of two threads executing a specific code path at a time.


internal sealed class SimpleHybridLock : IDisposable 
{
   // The Int32 is used by the primitive user-mode constructs (Interlocked methods)
   private Int32 m_waiters = 0;
   
   // The AutoResetEvent is the primitive kernel-mode construct
   private readonly AutoResetEvent m_waiterLock = new AutoResetEvent(false);
   
   public void Enter() 
   {
      // Indicate that this thread wants the lock
      if (Interlocked.Increment(ref m_waiters) == 1)
          return; // Lock was free, no contention, just return
		  
      // Another thread has the lock (contention), make this thread wait
      m_waiterLock.WaitOne(); // Bad performance hit here
	  
      // When WaitOne returns, this thread now has the lock
   }
   
   public void Leave() 
   {
      // This thread is releasing the lock
      if (Interlocked.Decrement(ref m_waiters) == 0)
          return; // No other threads are waiting, just return
		  
      // Other threads are waiting, wake 1 of them
      m_waiterLock.Set(); // Bad performance hit here
   }
   
   public void Dispose() { m_waiterLock.Dispose(); }
}

In addition, some locks impose a limitation where the thread 
that acquires the lock must be the thread that releases the lock.

internal sealed class AnotherHybridLock : IDisposable 
{
    // The Int32 is used by the primitive user-mode constructs (Interlocked methods)
    private Int32 m_waiters = 0;
	
    // The AutoResetEvent is the primitive kernel-mode construct
    private AutoResetEvent m_waiterLock = new AutoResetEvent(false);
	
    // This field controls spinning in an effort to improve performance
    private Int32 m_spincount = 4000; // Arbitrarily chosen count
	
    // These fields indicate which thread owns the lock and how many times it owns it
    private Int32 m_owningThreadId = 0, m_recursion = 0;
	
    public void Enter() 
	{
        // If calling thread already owns the lock, increment recursion count and return
        Int32 threadId = Thread.CurrentThread.ManagedThreadId;
		
        if (threadId == m_owningThreadId) 
		{ 
		   m_recursion++; 
		   return; 
		}
		
        // The calling thread doesn't own the lock, try to get it
        SpinWait spinwait = new SpinWait();
	   
        for (Int32 spinCount = 0; spinCount < m_spincount; spinCount++) 
		{
           // If the lock was free, this thread got it; 
		   // set some state and return
           if (Interlocked.CompareExchange(ref m_waiters, 1, 0) == 0) 
		       goto GotLock;
			   
           // Black magic: give other threads a chance to run
           // in hopes that the lock will be released
           spinwait.SpinOnce();
        }
		
        // Spinning is over and the lock was still not obtained, 
		// try one more time
        if (Interlocked.Increment(ref m_waiters) > 1) 
		{
           // Still contention, this thread must wait
           m_waiterLock.WaitOne(); // Wait for the lock; performance hit
           // When this thread wakes, it owns the lock; set some state and return
        }
		
        GotLock:
		
        // When a thread gets the lock, we record its ID and
        // indicate that the thread owns the lock once
        m_owningThreadId = threadId; 
		m_recursion = 1;
    }
	
	public void Leave() 
	{
        // If the calling thread doesn't own the lock, there is a bug
        Int32 threadId = Thread.CurrentThread.ManagedThreadId;
		
        if (threadId != m_owningThreadId)
            throw new SynchronizationLockException("Lock not owned by calling thread");
			
		// Decrement the recursion count. If this thread still owns the lock, just return
        if (--m_recursion > 0) 
		    return;
			
        m_owningThreadId = 0;   // No thread owns the lock now
		
        // If no other threads are waiting, just return
        if (Interlocked.Decrement(ref m_waiters) == 0)
            return;
			
        // Other threads are waiting, wake 1 of them
        m_waiterLock.Set(); // Bad performance hit here
    }
	
    public void Dispose() { m_waiterLock.Dispose(); }
}


62. Examples of hybrid syncronisation locks in .NET framework?

ManualResetEventSlim, SemaphoreSlim, 

When the CLR initializes, it allocates an array of sync blocks in native heap.
whenever an object is created in the heap, it gets two additional overhead fields associated
with it. The first overhead field, the type object pointer, contains the memory address of the type’s
type object. The second overhead field, the sync block index, contains an integer index into the array
of sync blocks. 

When an object is constructed, the object’s sync block index is initialized to -1, which indicates
that it doesn’t refer to any sync block. Then, when Monitor.Enter is called, the CLR finds a free sync
block in the array and sets the object’s sync block index to refer to the sync block that was found.

Problems with locks:

- When you call Monitor’s methods, passing a reference to a proxy object, 
  you are locking the proxy object, not the actual object that the proxy refers to.
  
- If a thread calls Monitor.Enter, passing it a reference to a type object 
  that has been loaded domain neutral (discussed in Chapter 22), 
  the thread is taking a lock on that type across all AppDomains in the process.

- When passing a string across an AppDomain boundary, the CLR does not make a copy of
  the string; instead, it simply passes a reference to the string into the other AppDomain. This
  improves performance, and in theory, it should be OK because String objects are immutable.
  However, like all objects, String objects have a sync block index associated with them, which
  is mutable, and this allows threads in different AppDomains to synchronize with each other
  unknowingly.
  
- Because Monitor’s methods take an Object, passing a value type causes the value type to
  get boxed, resulting in the thread taking a lock on the boxed object. Each time Monitor.Enter
  is called, a lock is taken on a completely different object 
  and you get no thread synchronization at all.

- Applying the [MethodImpl(MethodImplOptions.Synchronized)] attribute to a method
  causes the JIT compiler to surround the method’s native code 
  with calls to Monitor.Enter and Monitor.Exit. 
  If the method is an instance method, then this is passed to these methods,
  locking the implicitly public lock. 
  If the method is static, then a reference to the type’s type object is passed to these methods, 
  potentially locking a domain-neutral type. The recommendation is to never use this attribute.
  
- When calling a type’s type constructor (discussed in Chapter 8, “Methods”), 
  the CLR takes a lock on the type’s type object to ensure 
  that only one thread initializes the type object and its static fields. 
  Again, this type could be loaded domain neutral, causing a problem. 
  For example, if the type constructor’s code enters an infinite loop, 
  then the type is unusable by all App-Domains in the process. 
  The recommendation here is to avoid type constructors 
  as much as possible or least keep them short and simple.
  

private void SomeMethod() 
{
   lock (this) 
   {
       // This code has exclusive access to the data...
   }
}

It is equivalent to having written the method like this.
private void SomeMethod() 
{
    Boolean lockTaken = false;
	
    try 
	{
       // An exception (such as ThreadAbortException) could occur here...
       Monitor.Enter(this, ref lockTaken);
	   
       // This code has exclusive access to the data...
    }
    finally 
	{
       if (lockTaken) 
	       Monitor.Exit(this);
    }
}


However, this is not a good thing. 
If an exception occurs inside the try block while changing state, 
then the state is now corrupted. 
When the lock is exited in the finally block, 
another thread will now start manipulating the corrupted state. 
It is better to have your application hang 
than it is to continue running with a corrupted state and potential security holes.


63. What is ReadWrite lock and benefits of using?

However, if all the threads want to access the data
in a read-only fashion, then there is no need to block them at all; 
they should all be able to access the data concurrently. 
On the other hand, if a thread wants to modify the data, 
then this thread needs exclusive access to the data. 
The ReaderWriterLockSlim construct encapsulates the logic to solve this problem. 
Specifically, the construct controls threads like this:

- When one thread is writing to the data, 
  all other threads requesting access are blocked.

- When one thread is reading from the data, other threads requesting read access 
  are allowed to continue executing, 
  but threads requesting write access are blocked.
  
- When a thread writing to the data has completed, 
  either a single writer thread is unblocked
  so it can access the data or all the reader threads are unblocked 
  so that all of them can access the data concurrently. 
  If no threads are blocked, then the lock is free and available 
  for the next reader or writer thread that wants it.

- When all threads reading from the data have completed, 
  a single writer thread is unblocked so it can access the data. 
  If no threads are blocked, then the lock is free 
  and available for the next reader or writer thread that wants it.
  
Reader-Writer lock in use:

internal sealed class Transaction : IDisposable 
{
   private readonly ReaderWriterLockSlim m_lock = 
                new ReaderWriterLockSlim(LockRecursionPolicy.NoRecursion);
				
   private DateTime m_timeOfLastTrans;
   
   public void PerformTransaction() 
   {
      m_lock.EnterWriteLock();
      // This code has exclusive access to the data...
      m_timeOfLastTrans = DateTime.Now;
      m_lock.ExitWriteLock();
   }
   
   public DateTime LastTransaction 
   {
      get 
	  {
          m_lock.EnterReadLock();
		  
          // This code has shared access to the data...
          DateTime temp = m_timeOfLastTrans;
          m_lock.ExitReadLock();
		  
          return temp;
      }
   }
   
   public void Dispose() { m_lock.Dispose(); }

}

Now I’d like to give you a sense of how it works. 
Internally, the class has an Int32 field for the state of the lock, 
a Semaphore object that reader threads block on, 
and an AutoResetEvent object that writer threads block on. 
The Int64 state field is divided into five subfields as follows:

Four bits represent the state of the lock itself. 
The possibilities are 

0=Free, 
1=OwnedByWriter,
2=OwnedByReaders, 
3=OwnedByReadersAndWriterPending, 
4=ReservedForWriter

The other values are not used.

Twenty bits (a number from 0 to 1,048,575) represent 
the number of reader threads reading (RR) 
that the lock has currently allowed in.

Twenty bits represent the number of reader threads 
waiting (RW) to get into the lock. 
These threads block on the auto-reset event object.

Twenty bits represent the number of reader threads 
waiting (RW) to get into the lock. 
These threads block on the auto-reset event object.

Twenty bits represent the number of writer threads 
waiting (WW) to get into the lock. 
These threads block on the other semaphore object.

- Thread entrers a lock for shared access:
1. If the lock is Free: Set state to OwnedByReaders, RR=1, Return.
2. If the lock is OwnedByReaders: RR++, Return.
3. If the lock is OwnedByWriters: RW++. Block reader thread. 
   When the thread wakes, loop around and try again.

- Thread that has shared access leaves the lock:
1. RR--.
2. If RR > 0: Return
3. If WW > 0: 
    Set state to ReservedForWriter, WW--, 
	Release 1 blocked writer thread, Return
4. If RW == 0 && WW == 0: Set state to Free, Return

- Thread enters the lock for exclusive access:
1. If the lock is Free: Set state to OwnedByWriter, Return.
2. If the lock is ReservedForWriter: Set state to OwnedByWriter, Return.
3. If the lock is OwnedByWriter: WW++, Block writer thread. 
   When thread wakes, loop around and try again.
4. Set state to OwnedByReadersAndWriterPending, WW++, Block writer thread. 
   When thread wakes, loop around and try again.
   
- Thread that has exclusive access leaves the lock:
1. If WW == 0 && RW == 0: Set state to Free, Return
2. If WW > 0: Set state to ReservedForWriter, WW--, 
   Release 1 blocked writer thread, Return.
3. If WW == 0 && RW > 0: Set state to Free , 
   RW=0, Wake all blocked reader threads, Return


64. Double-Check Locking Technique?

internal sealed class Singleton 
{
   // s_lock is required for thread safety and having this object assumes that creating
   // the singleton object is more expensive than creating a System.Object object and that
   // creating the singleton object may not be necessary at all. Otherwise, it is more
   // efficient and easier to just create the singleton object in a class constructor
   private static readonly Object s_lock = new Object();
   
   // This field will refer to the one Singleton object
   private static Singleton s_value = null;
   
   // Private constructor prevents any code outside this class from creating an instance
   private Singleton() 
   {
      // Code to initialize the one Singleton object goes here...
   }
   
   // Public, static method that returns the Singleton object (creating it if necessary)
   public static Singleton GetSingleton() 
   {
      // If the Singleton was already created, just return it (this is fast)
      if (s_value != null) 
	      return s_value;
		  
      Monitor.Enter(s_lock); // Not created, let 1 thread create it
	  
      if (s_value == null) 
	  {
          // Still not created, create it
          Singleton temp = new Singleton();
		  
          // Save the reference in s_value (see discussion for details)
          Volatile.Write(ref s_value, temp);
      }
	  
      Monitor.Exit(s_lock);

      return s_value;    
   }
}

In the CLR, calling any lock method is a full memory fence, 
and any variable writes you have before the fence 
must complete before the fence and any variable reads after the fence must start after it.

What is memory fence?
Instruction to a processor to refresh its cache lines.
If a processor does reads and writes:


a1 =  var1 == 4;
a1 =  var1 == 10;

var2 = 10;
var2 = 15;

memory fence:

a2 = var2 == 15;
var1 = 7;

We have one read before memory fence.
repeated reads of variable var1 before 
the memory fence take var1 from register

Reads before memory fence look like:


load var1 from memory to register1
register1 = 3.
compare register1 with 4
compare register2 witn 10.

after mem fence

load var1 from memory to register1
compare register1 with 7

Without memory fence we do not do reloading of a var1
from memory to a register.

Writes before the memory fence look like:

load2 var2 from memory to register2
set register2 to 10
set register2 to 15

write from register2 to memory var2.
memory barrier


For the GetSingleton method, this means that the s_value field 
must be reread after the call to Monitor.Enter; 
it cannot be cached in a register across this method call.

The problems with this double-checking lock:

1. s_value is cached in a processor's register
   
 if (s_value != null) 
	 return s_value;
	 
 Monitor.Enter(s_lock); 
	 
 if (s_value == null) 
 {  }
 
 read s_value and it is null 
 then pause
 check if it is null again.
 if it is chached, it will always be null
 
 Monitor.Enter puts a full memory fence. 
 And this fixes the problem.
 
2. Compile optimization.


      Monitor.Enter(s_lock); // Not created, let 1 thread create it
	  
      if (s_value == null) 
	  {
	      s_value = new Singleton();
      }
	  
      Monitor.Exit(s_lock);

 The compiler can generate the following code:
 
 allocate new memory.
 set s_value to point to this memory.
 call constructor and initialize memory internals.
 
 But this is not the desired behaviour because 
 when s_value becomes not null another thread takes this as singleton object
 and this object becomes invalid.
 
 the desired behaviour is:
 
 allocate new memory.
 call constructor and initialize memory internals.
 set s_value to point to this memory.
 
 
  // Still not created, create it
  Singleton temp = new Singleton();
		  
  // Save the reference in s_value (see discussion for details)
  Volatile.Write(ref s_value, temp);
 
 
 Volatile Write prevents code rearrangements
 by C# .NET code optimization
 It generates full memory barrier before and all
 writes will be done into memory before volatile write.
 
 internal sealed class Singleton 
 { 
     private static Singleton s_value = new Singleton();
	 
     // Private constructor prevents any code outside this class from creating an instance
     private Singleton() 
	 {
        // Code to initialize the one Singleton object goes here...
     }
	 
     // Public, static method that returns the Singleton object (creating it if necessary)
     public static Singleton GetSingleton() { return s_value; }
}


Because the CLR automatically calls a type’s class constructor 
the first time code attempts to access a member of the class, 
the first time a thread queries Singleton’s GetSingleton method, 
the CLR will automatically call the class constructor, 
which creates an instance of the object. 
Furthermore, the CLR already ensures that calls 
to a class constructor are thread safe. 

internal sealed class Singleton 
{
    private static Singleton s_value = null;
	
    // Private constructor prevents any code outside this class from creating an instance
    private Singleton() 
	{
       // Code to initialize the one Singleton object goes here...
    }
	
    // Public, static method that returns the Singleton object (creating it if necessary)
    public static Singleton GetSingleton() 
	{
       if (s_value != null) return s_value;
	   
	   // Create a new Singleton and root it 
	   // if another thread didn't do it first
       Singleton temp = new Singleton();
	   
       Interlocked.CompareExchange(ref s_value, temp, null);
	   
	   // If this thread lost, then the second Singleton object gets GC'd
       return s_value; // Return reference to the single object
	}
}


65. Condition variable pattern.

internal sealed class ConditionVariablePattern 
{
    private readonly Object m_lock = new Object();
    private Boolean m_condition = false;
	
    public void Thread1() 
	{ 
        Monitor.Enter(m_lock); // Acquire a mutual-exclusive lock
		
        // While under the lock, test the complex condition "atomically"
        while (!m_condition) 
		{
            // If condition is not met, wait for another thread to change the condition
            Monitor.Wait(m_lock); // Temporarily release lock so other threads can get it
        }
		
        // The condition was met, process the data...
        Monitor.Exit(m_lock); // Permanently release lock
    }
	
    public void Thread2() 
	{
        Monitor.Enter(m_lock);      // Acquire a mutual-exclusive lock
		
        // Process data and modify the condition...
        m_condition = true;
		
        // Monitor.Pulse(m_lock);   // Wakes one waiter AFTER lock is released
        Monitor.PulseAll(m_lock);   // Wakes all waiters AFTER lock is released
		
        Monitor.Exit(m_lock); // Release lock
}
}

If the condition is false, then you want the thread to spin on the condition, 
but spinning wastes CPU time, so instead, the thread calls Wait. 
Wait releases the lock so that another thread can get it and blocks the calling thread.

The Thread2 method shows code that the second thread executes. 
It calls Enter to take ownership of the lock, processes some data, 
which results in changing the state of the condition, 
and then calls Pulse or PulseAll, which will unblock a thread from its Wait call. 
Pulse unblocks the longest waiting thread (if any), 
whereas PulseAll unblocks all waiting threads (if any).


internal sealed class SynchronizedQueue<T> 
{
    private readonly Object   m_lock = new Object();
    private readonly Queue<T> m_queue = new Queue<T>();
	
    public void Enqueue(T item) 
	{
        Monitor.Enter(m_lock);
		
        // After enqueuing an item, wake up any/all waiters
        m_queue.Enqueue(item);
        Monitor.PulseAll(m_lock);
		
        Monitor.Exit(m_lock);
    }
	
    public T Dequeue() 
	{
        Monitor.Enter(m_lock);
		
        // Loop while the queue is empty (the condition)
        while (m_queue.Count == 0)
            Monitor.Wait(m_lock);
			
        // Dequeue an item from the queue and return it for processing
        T item = m_queue.Dequeue();
		
        Monitor.Exit(m_lock);
        return item;
    }
}

66. What is TaskCompletionSource?

Earlier I showed the code for a client application 
that makes a request over a named pipe. 
Let me show the server side of this code now.


private static async void StartServer() 
{
    while (true) 
	{
        var pipe = new NamedPipeServerStream(
		                 c_pipeName, 
						 PipeDirection.InOut, 
						 -1,
                         PipeTransmissionMode.Message, 
						 PipeOptions.Asynchronous | PipeOptions.WriteThrough);
						 
        // Asynchronously accept a client connection
        // NOTE: NamedPipServerStream uses the old Asynchronous Programming Model (APM)
        // I convert the old APM to the new Task model via TaskFactory's FromAsync method
		
        await Task.Factory.FromAsync(
		                 pipe.BeginWaitForConnection, 
						 pipe.EndWaitForConnection, 
						 null);
		
        // Start servicing the client, which returns immediately because it is asynchronous
        ServiceClientRequestAsync(pipe);
    }
}


For the old event-based programming model, the FCL 
does not include any helper methods to adapt this model into the new Task-based model. 
So you have to hand-code it. 
Here is code demonstrating how to wrap a WebClient 
(which uses the event-based programming model) 
with a TaskCompletionSource so it can be awaited on in an async function.


private static async Task<String> AwaitWebClient(Uri uri) 
{
    // The System.Net.WebClient class supports the Event-based Asynchronous Pattern
    var wc = new System.Net.WebClient();
	
    // Create the TaskCompletionSource and its underlying Task object
    var tcs = new TaskCompletionSource<String>();
	
    // When a string completes downloading, the WebClient object raises the
    // DownloadStringCompleted event, which completes the TaskCompletionSource
	
    wc.DownloadStringCompleted += (s, e) => 
	{
        if (e.Cancelled) 
	        tcs.SetCanceled();
	    else if (e.Error != null) 
		    tcs.SetException(e.Error);
        else 
		    tcs.SetResult(e.Result);
    };
	
    // Start the asynchronous operation
    wc.DownloadStringAsync(uri);
	
    // Now, we can the TaskCompletionSource’s Task 
	// and process the result as usual
    String result = await tcs.Task;
	
    // Process the resulting string (if desired)...
    return result;
}




66. Asyncronous blocking constructions.

Locks are popular but, when held for a long time, 
they introduce significant scalability issues. 
What would really be useful is if we had asynchronous synchronization constructs 
where your code indicates that it wants a lock.
If the thread can’t have it, it can just return and perform some other work,
rather than blocking indefinitely.
Then, when the lock becomes available, 
your code somehow gets resumed, 
so it can access the resource that the lock protects.

public Task<Boolean> WaitAsync(Int32 millisecondsTimeout, CancellationToken cancellationToken);

With this, you can synchronize access to a resource asynchronously 
(without blocking any thread).

private static async Task AccessResourceViaAsyncSynchronization(SemaphoreSlim asyncLock) 
{
    // TODO: Execute whatever code you want here...
    await asyncLock.WaitAsync(); // Request exclusive access to a resource via its lock
   
    // When we get here, we know that no other thread is accessing the resource
    // TODO: Access the resource (exclusively)...
    // When done accessing resource, relinquish lock so other code can access the resource
    asyncLock.Release();
	
    // TODO: Execute whatever code you want here...
}

private static void ConcurrentExclusiveSchedulerDemo() 
{
    var cesp = new ConcurrentExclusiveSchedulerPair();
    var tfExclusive = new TaskFactory(cesp.ExclusiveScheduler);
    var tfConcurrent = new TaskFactory(cesp.ConcurrentScheduler);
    
    for (Int32 operation = 0; operation < 5; operation++) 
	{
        var exclusive = operation < 2; // For demo, I make 2 exclusive & 3 concurrent
		
       (exclusive ? tfExclusive : tfConcurrent).StartNew(() => 
	   {
           Console.WriteLine("{0} access", exclusive ? "exclusive" : "concurrent");
           // TODO: Do exclusive write or concurrent read computation here...
       });
    }	
}

private static async Task AccessResourceViaAsyncSynchronization(AsyncOneManyLock asyncLock) 
{
    // TODO: Execute whatever code you want here...
    // Pass OneManyMode.Exclusive or OneManyMode.Shared for wanted concurrent access
    await asyncLock.AcquireAsync(OneManyMode.Shared); // Request shared access
	
    // When we get here, no threads are writing to the resource; other threads may be reading
    // TODO: Read from the resource...
    // When done accessing resource, relinquish lock so other code can access the resource
    asyncLock.Release();
	
    // TODO: Execute whatever code you want here...
}


public sealed class AsyncOneManyLock 
{
   #region Lock code
   
   private SpinLock m_lock = new SpinLock(true);     // Don't use readonly with a SpinLock
   
   private void Lock() 
   { 
       Boolean taken = false; 
	   m_lock.Enter(ref taken); 
   }
   
   private void Unlock() 
   { 
       m_lock.Exit(); 
   }
   
   #endregion
   
   #region Lock state and helper methods
   
   private Int32   m_state = 0;
   private Boolean IsFree                { get { return m_state == 0; } }
   private Boolean IsOwnedByWriter       { get { return m_state == -1; } }
   private Boolean IsOwnedByReaders      { get { return m_state > 0; } }
   private Int32 AddReaders(Int32 count) { return m_state += count; }
   
   private Int32 SubtractReader() { return --m_state; }
   private void MakeWriter()      { m_state = -1; }
   private void MakeFree()        { m_state = 0; }
   
   #endregion
   
   // For the no-contention case to improve performance and reduce memory consumption
   private readonly Task m_noContentionAccessGranter;
   
   // Each waiting writers wakes up via their own TaskCompletionSource queued here
   private readonly Queue<TaskCompletionSource<Object>> m_qWaitingWriters =
                   new Queue<TaskCompletionSource<Object>>();
				   
   // All waiting readers wake up by signaling a single TaskCompletionSource
   private TaskCompletionSource<Object> m_waitingReadersSignal =
                          new TaskCompletionSource<Object>();
						  
   private Int32 m_numWaitingReaders = 0;
   
   public AsyncOneManyLock() 
   {
      m_noContentionAccessGranter = Task.FromResult<Object>(null);
   }    
   
   public Task WaitAsync(OneManyMode mode) 
   {
      Task accressGranter = m_noContentionAccessGranter; // Assume no contention
      Lock();
	  
      switch (mode) 
	  {
          case OneManyMode.Exclusive:
              if (IsFree) 
		      {
                  MakeWriter(); // No contention
              }    
		      else 
		      {
                  // Contention: Queue new writer task & return it so writer waits
			  
                  var tcs = new TaskCompletionSource<Object>();
                  m_qWaitingWriters.Enqueue(tcs);
                  accressGranter = tcs.Task;
              }
              break;
			  
          case OneManyMode.Shared:
              if (IsFree || (IsOwnedByReaders && m_qWaitingWriters.Count == 0)) 
			  {
                  AddReaders(1); // No contention
              } 
			  else 
			  { 
			      // Contention
                  // Contention: Increment waiting readers & return reader task so reader waits
                  m_numWaitingReaders++;
                  accressGranter = m_waitingReadersSignal.Task.ContinueWith(t => t.Result);
              }
              break;
      }
	   
      Unlock();
	  
      return accressGranter;
   }
   
   public void Release() 
   {
       TaskCompletionSource<Object> accessGranter = null; // Assume no code is released
	   
       Lock();
	   
       if (IsOwnedByWriter) 
	       MakeFree(); // The writer left
       else 
	       SubtractReader(); // A reader left
		   
       if (IsFree) 
	   {
           // If free, wake 1 waiting writer or all waiting readers
           if (m_qWaitingWriters.Count > 0) 
		   {
               MakeWriter();
               accessGranter = m_qWaitingWriters.Dequeue();
           } 
		   else if (m_numWaitingReaders > 0) 
		   {
               AddReaders(m_numWaitingReaders);
               m_numWaitingReaders   = 0;
               accessGranter         = m_waitingReadersSignal;
			   
               // Create a new TCS for future readers that need to wait
               m_waitingReadersSignal = new TaskCompletionSource<Object>();
           }
       }
	   
	   Unlock();
	   
	   // Wake the writer/reader outside the lock to reduce
       // chance of contention improving performance
       if (accessGranter != null) 
	       accessGranter.SetResult(null);
   }
}


67. Concurrent collection classes.

ConcurrentQueue<T>, 
ConcurrentStack<T>,
ConcurrentBag<T>,
ConcurrentDictionary<TKey, TValue>,

All these collection classes are non-blocking. 
That is, if a thread tries to extract an element 
when no such element exists, the thread returns immediately; 

the thread does not block waiting for an element to appear. 
This is why methods like TryDequeue, TryPop, TryTake, and TryGetValue 
all return true if an item was obtained and returns false, if not.


68. is, as?

The is operator checks whether an
object is compatible with a given type, and the result of the evaluation is a Boolean: true or false.
The is operator will never throw an exception.


69. How Things Relate at Run Time?

Thread stack:

void M1()
{
   // some other local variables
   var1
   var2
   var3
   String name = "Joe";
   M2(name);
   doit();
   ...
   return;
}

void M2(string s)
{
   int length = s.Length;
   int tally;
   ...
   return;
}

--before calling M2:

Stack
var1
var2
var3
name

--going into method M2:

Stack
var1
var2
var3
name
s     //exact copy of name
return address [doIt method]

--inside method:

Stack
var1
var2   // M1 locals
var3
name
s      // exact copy of name
return address [doIt method]
length
tally  // M2 locals.

--when return from M2:
return to doIt method.
and delete all M2 stack variables:

var1
var2   // M1 locals
var3
name

internal class Employee 
{
   public Int32 GetYearsEmployed() 
   { 
     ... 
   }
   
   public virtual String GetProgressReport() 
   { 
     ... 
   }
   
   public static Employee Lookup(String name) 
   { 
     ... 
   }
}

internal sealed class Manager : Employee 
{
   public override String GetProgressReport() 
   { 
     ... 
   }
}

void M3() 
{
   Employee e;
   Int32 year;
   
   e = new Manager();
   e = Employee.Lookup("Joe");
   
   year = e.GetYearsEmployed();
   e.GetProgressReport();
}

It notices all of the types that are referred to inside M3: 
Employee, Int32, Manager, and String (because of "Joe"). 

Then, using the assembly’s metadata, 
the CLR extracts information about these types 
and creates some data structures to represent the types themselves.

When M3’s prologue code executes, memory for 
the local variables must be allocated from the thread’s stack,

stack:
e = null
year = 0

e = new Manager();
allocates memory in a heap 
and set e to this address.

e = Employee.Lookup("Joe");
it creates new Manager object and sets it
to e variable. So the previous manager is no
longer being referenced and can be garbage collected.


When calling a virtual instance method, 
the JIT compiler produces some additional code in the method,
which will be executed each time the method is invoked. 

This code will first look in the variable being used 
to make the call and then follow the address to the calling object. 
In this case, the variable e points to the Manager object representing “Joe.”


69. Overflow?


UInt32 invalid = unchecked((UInt32) (-1)); // OK

And here is an example that uses the checked operator.

Byte b = 100;
b      = checked((Byte) (b + 200)); // OverflowException is thrown

checked 
{ 
   // Start of checked block
   Byte b = 100;
   b = (Byte) (b + 200); // This expression is checked for overflow.
} 
// End of checked block


70. Reference Types and Value Types?

Reference Types drawbacks:
- The memory must be allocated from the managed heap.
- Each object allocated on the heap has some additional overhead members associated with it
that must be initialized.
- The other bytes in the object (for the fields) are always set to zero.
- Allocating an object from the managed heap could force a garbage collection to occur.

Even though you can’t choose a base type when defining your own value type, a value type
can implement one or more interfaces if you choose. In addition, all value types are sealed, which
prevents a value type from being used as a base type for any other reference type or value type. So,
for example, it’s not possible to define any new types using Boolean, Char, Int32, UInt64, Single,
Double, Decimal, and so on as base types.

// Reference type (because of 'class')
class SomeRef { public Int32 x; }

// Value type (because of 'struct')
struct SomeVal { public Int32 x; }

static void ValueTypeDemo() 
{
    SomeRef r1 = new SomeRef(); // Allocated in heap
    SomeVal v1 = new SomeVal(); // Allocated on stack

	r1.x = 5;   // Pointer dereference
    v1.x = 5;   // Changed on stack
	
	SomeRef r2 = r1; // Copies reference (pointer) only
    SomeVal v2 = v1; // Allocate on stack & copies members
	
	r1.x = 8;   // Changes r1.x and r2.x
    v1.x = 9;   // Changes v1.x, not v2.x
}

SomeVal  v1 = new SomeVal(); // Allocated on stack

The preceding line could have been written like this instead.
SomeVal  v1;    // Allocated on stack

// These two lines compile because C# thinks that
// v1's fields have been initialized to 0.
SomeVal v1 = new SomeVal();
Int32   a = v1.x;

// These two lines don't compile because C# doesn't think that
// v1's fields have been initialized to 0.
SomeVal v1;
Int32   a = v1.x; // error CS0170: Use of possibly unassigned field 'x'


In particular, you should declare a type as a value type 
if all the following statements are true:

- The type acts as a primitive type. 
  Specifically, this means that it is a fairly simple type 
  that has no members that modify any of its instance fields. 
  When a type offers no members that alter its fields, 
  we say that the type is immutable.
  
- The type doesn’t need to inherit from any other type.

- The type won’t have any other types derived from it.

- Instances of the type are small (approximately 16 bytes or less).

- Instances of the type are large (greater than 16 bytes) 
  and are not passed as method parameters or returned from methods.

- Because you can’t define a new value type or a new reference type 
  by using a value type as a base class, 
  you shouldn’t introduce any new virtual methods into a value type. 
  No methods can be abstract, and all methods are implicitly sealed 
  (can’t be overridden).
  
Because a value type variable isn’t a pointer, 
it’s not possible to generate a NullReferenceException
when accessing a value type. The CLR does offer a special feature that
adds the notion of nullability to a value type.

When you assign a value type variable to another value type variable, 
a field-by-field copy is made. 
When you assign a reference type variable to another reference type variable, 
only the memory address is copied.


71. How the CLR Controls the Layout of a Type’s Fields?

To improve performance, the CLR is capable of arranging the fields of a type any way it chooses.
For example, the CLR might reorder fields in memory so that object references are grouped
together and data fields are properly aligned and packed. However, when you define a type,
you can tell the CLR whether it must keep the type’s fields in the same order as the developer
specified them or whether it can reorder them as it sees fit.

To this attribute’s constructor, 
you can pass:
 
LayoutKind.Auto         to have the CLR arrange the fields, 
LayoutKind.Sequential   to have the CLR preserve your field layout, or LayoutKind.
LayoutKind.Explicit     to explicitly arrange the fields in memory by using offsets.

You should be aware that Microsoft’s C# compiler 
selects LayoutKind.Auto for reference types (classes) 
and LayoutKind.Sequential for value types (structures).


72. Boxing and unboxing?

// Declare a value type.
struct Point 
{
    public Int32 x, y;
}

public static void Main() 
{
    ArrayList a = new ArrayList();
    Point p;                      // Allocate a Point (not in the heap).
	
    for (Int32 i = 0; i < 10; i++) 
	{
        p.x = p.y = i; // Initialize the members in the value type.
        a.Add(p);      // Box the value type and add the
        // reference to the Arraylist.
    }
    ...
}

For this code to work, the Point value type
must be converted into a true heap-managed object, 
and a reference to this object must be obtained.

It’s possible to convert a value type to a reference type 
by using a mechanism called boxing. 
Internally, here’s what happens 
when an instance of a value type is boxed:

  1. Memory is allocated from the managed heap. 
     The amount of memory allocated is the size
     required by the value type’s fields plus 
	 the two additional overhead members 
	 (the type object pointer and the sync block index) 
	 required by all objects on the managed heap.
	 
  2. The value type’s fields are copied to the newly allocated heap memory.
  
  3. The address of the object is returned. 
     This address is now a reference to an object; 
	 the value type is now a reference type.
	 
But one of the biggest improvements
is that the generic collection classes 
allow you to work with collections of value types
without requiring that items in the collection be boxed/unboxed.

Now that you know how boxing works, let’s talk about unboxing. 
Let’s say that you want to grab the first element 
out of the ArrayList by using the following code.

Point p = (Point) a[0];

For this to work, all of the fields contained in the boxed
Point object must be copied into the value type variable, 
p, which is on the thread’s stack.

The CLR accomplishes this copying in two steps. 
First, the address of the Point fields 
in the boxed Point object is obtained. 
This process is called unboxing. 
Then, the values of these fields are copied 
from the heap to the stack-based value type instance.

public static void Main() 
{
    Int32  v = 5;    // Create an unboxed value type variable.
    Object o = v;    // o refers to a boxed Int32 containing 5.
    v = 123;         // Changes the unboxed value to 123
    Console.WriteLine(v + ", " + (Int32) o); // Displays "123, 5"
}

// Box v and leave the pointer on the stack for Concat.
IL_000c: ldloc.0
IL_000d: box [mscorlib]System.Int32

// Unbox o: Get the pointer to the In32's field on the stack.
IL_0017: ldloc.1
IL_0018: unbox.any [mscorlib]System.Int32

// Box the Int32 and leave the pointer on the stack for Concat.
IL_001d: box [mscorlib]System.Int32

WriteLine wants a String object passed to it, 
but there is no string object. 
Instead, these three items are available: 
  an unboxed Int32 value type instance (v), 
  a String (which is a reference type), 
  and a reference to a boxed Int32 value type instance (o) 
  that is being cast to an unboxed Int32.

public static String Concat(Object arg0, Object arg1, Object arg2);

For the first parameter, arg0, v is passed. 
But v is an unboxed value parameter 
and arg0 is an Object, so v must be boxed 
and the address to the boxed v is passed for arg0.

Finally, for the arg2 parameter,
o (a reference to an Object) is cast to an Int32. 
This requires an unboxing operation (but no copy operation), 
which retrieves the address of the unboxed Int32 
contained inside the boxed Int32.
This unboxed Int32 instance must be boxed again 
and the new boxed instance’s memory address passed for Concat’s arg2 parameter.

However, if your override of the virtual method calls 
into the base type's implementation of the method, 
then the value type instance does get boxed 
when calling the base type's implementation 
so that a reference to a heap object gets passed 
to the this pointer into the base method.

However, calling a nonvirtual inherited method 
(such as GetType or MemberwiseClone) 
always requires the value type to be boxed 
because these methods are defined by System.Object, 
so the methods expect the this argument to be 
a pointer that refers to an object on the heap.

In addition, casting an unboxed instance of a value type 
to one of the type’s interfaces requires 
the instance to be boxed, because interface variables 
must always contain a reference to an object on the heap.

internal struct Point : IComparable 
{
    private readonly Int32 m_x, m_y;
	
    // Constructor to easily initialize the fields
    public Point(Int32 x, Int32 y) 
	{
        m_x = x;
        m_y = y;
    }
	
    // Override ToString method inherited from System.ValueType
    public override String ToString() 
	{
       // Return the point as a string. Note: calling ToString prevents boxing
       return String.Format("({0}, {1})", m_x.ToString(), m_y.ToString());
    }
	
    // Implementation of type-safe CompareTo method
    public Int32 CompareTo(Point other) 
	{
       // Use the Pythagorean Theorem to calculate
       // which point is farther from the origin (0, 0)
       return Math.Sign(Math.Sqrt(m_x * m_x + m_y * m_y)
            - Math.Sqrt(other.m_x * other.m_x + other.m_y * other.m_y));
    }
	
	// Implementation of IComparable's CompareTo method
    public Int32 CompareTo(Object o) 
	{
       if (GetType() != o.GetType()) 
	   {
           throw new ArgumentException("o is not a Point");
       }
	   
       // Call type-safe CompareTo method
       return CompareTo((Point) o);
    }
}


// Create two Point instances on the stack.
Point p1 = new Point(10, 10);
Point p2 = new Point(20, 20);

// p1 does NOT get boxed to call ToString (a virtual method).
Console.WriteLine(p1.ToString());    // "(10, 10)"


At first, you’d think that p1 would have to be boxed 
because ToString is a virtual method 
that is inherited from the base type, System.ValueType. 
Normally, to call a virtual method, the CLR needs to determine
the object’s type in order to locate the type’s method table. 
Because p1 is an unboxed value type, there’s no type object pointer. 
However, the just-in-time (JIT) compiler sees 
that Point overrides the ToString method, 
and it emits code that calls ToString directly (nonvirtually)
without having to do any boxing.


// p DOES get boxed to call GetType (a non-virtual method).
Console.WriteLine(p1.GetType());   // "Point"

Calling GetType In the call to the nonvirtual GetType method, p1 does have to be boxed.
The reason is that the Point type inherits GetType from System.Object. 
So to call GetType, the CLR must use a pointer to a type object, 
which can be obtained only by boxing p1.

// p1 does NOT get boxed to call CompareTo.
// p2 does NOT get boxed because CompareTo(Point) is called.
Console.WriteLine(p1.CompareTo(p2));   // "-1"

In the first call to CompareTo, p1 doesn’t have to be boxed
because Point implements the CompareTo method, 
and the compiler can just call it directly.

// p1 DOES get boxed, and the reference is placed in c.
IComparable c = p1;
Console.WriteLine(c.GetType());   // "Point"

// p1 does NOT get boxed to call CompareTo.
// Because CompareTo is not being passed a Point variable,
// CompareTo(Object) is called, which requires a reference to
// a boxed Point.
// c does NOT get boxed because it already refers to a boxed Point.
Console.WriteLine(p1.CompareTo(c));    // "0"

// c does NOT get boxed because it already refers to a boxed Point.
// p2 does get boxed because CompareTo(Object) is called.
Console.WriteLine(c.CompareTo(p2));// "-1"

// c is unboxed, and fields are copied into p2.
p2 = (Point) c;



73. Changing Fields in a Boxed Value Type 
by Using Interfaces (and Why You Shouldn’t Do This)?

// Point is a value type.
internal struct Point 
{
    private Int32 m_x, m_y;
	
    public Point(Int32 x, Int32 y) 
	{
        m_x = x;
        m_y = y;
    }
	
    public void Change(Int32 x, Int32 y) 
	{
        m_x = x; 
		m_y = y;
    }
	
    public override String ToString() 
	{
        return String.Format("({0}, {1})", m_x.ToString(), m_y.ToString());
    }
}

public static void Main() 
{
    Point p = new Point(1, 1);
    Console.WriteLine(p);
	
    p.Change(2, 2);
    Console.WriteLine(p);
	
    Object o = p;
    Console.WriteLine(o);
	
    ((Point) o).Change(3, 3);
    Console.WriteLine(o);
}

   I want to call the Change method 
to update the fields in the boxed Point object. 
However, Object (the type of the variable o) 
doesn’t know anything about the Change method, 
so I must first cast o to a Point.

   Casting o to a Point unboxes o and
copies the fields in the boxed Point 
to a temporary Point on the thread’s stack!

   The m_x and m_y fields of this temporary point are changed to 3 and 3, 
but the boxed Point isn’t affected by this call to Change. 
When WriteLine is called the fourth time, (2, 2) is displayed again. 
Many developers do not expect this.

// Interface defining a Change method
internal interface IChangeBoxedPoint 
{
    void Change(Int32 x, Int32 y);
}

// Point is a value type.
internal struct Point : IChangeBoxedPoint 
{
    private Int32 m_x, m_y;
	
    public Point(Int32 x, Int32 y) 
	{
       m_x = x;
       m_y = y;
    }
	
    public void Change(Int32 x, Int32 y) 
	{
       m_x = x; m_y = y;
    }
	
    public override String ToString() 
	{
       return String.Format("({0}, {1})", m_x.ToString(), m_y.ToString());
    }
}

public static void Main() 
{
    Point p = new Point(1, 1);
    Console.WriteLine(p);                    (1)
	
    p.Change(2, 2); 
    Console.WriteLine(p);                    (2)
	
    Object o = p;
    Console.WriteLine(o);                    (3)
	
    ((Point) o).Change(3, 3);
    Console.WriteLine(o);                    (4)
	
    // Boxes p, changes the boxed object and discards it
    ((IChangeBoxedPoint) p).Change(4, 4);
    Console.WriteLine(p);                    (5)
	
	// Changes the boxed object and shows it
    ((IChangeBoxedPoint) o).Change(5, 5);
    Console.WriteLine(o);                    (6)
}

Fifth call of Change method:

In the first example, the unboxed Point, p, is cast to an IChangeBoxedPoint. 
This cast causes the value in p to be boxed. 
Change is called on the boxed value, which does change its m_x and m_y fields
to 4 and 4, but after Change returns, 
the boxed object is immediately ready to be garbage collected.
So the fifth call to WriteLine displays (2, 2). Many developers won’t expect this result.

In the last example, the boxed Point referred to by o is cast to an IChangeBoxedPoint. No boxing
is necessary here because o is already a boxed Point. Then Change is called, which does change
the boxed Point’s m_x and m_y fields. The interface method Change has allowed me to change the
fields in a boxed Point object! Now, when WriteLine is called, it displays (5, 5) as expected. The
purpose of this whole example is to demonstrate how an interface method is able to modify the fields
of a boxed value type. In C#, this isn’t possible without using an interface method.

Earlier in this chapter, I mentioned that value types should be immutable: that
is, they should not define any members that modify any of the type’s instance fields. 





















  
  






























